\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage{comment}
\usepackage{float}
\graphicspath{ {./images/} }
% \usepackage{subfig}
% \usepackage{textcomp}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage{tabularx}





\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}


\title{A Wearable Imminent Danger Detection and Avoidance System with Region-based Ground Segmentation}
\author{\uppercase{Zhongen Li}\authorrefmark{1}, 
\uppercase{Fanghao Song\authorrefmark{1}, 
Brian Clark\authorrefmark{2}  \authorrefmark{3}, 
Dustin Grooms\authorrefmark{4}, and Chang Liu}.\authorrefmark{1},
\IEEEmembership{Senior Member, IEEE}}
\address[1]{School of Electrical and Computer Engineering, Ohio University, Athens, OH 45701 USA}
\address[2]{Ohio Musculoskeletal and Neurological Institute, Ohio University, Athens, OH 45701 USA}
\address[3]{Department of Biomedical Sciences, Ohio University, Athens, OH 45701 USA}
\address[4]{College of Health Sciences and Professions, Ohio University, Athens, OH 45701 USA}
\markboth
{Zhongen Li \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Zhongen Li \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
\corresp{Corresponding author: Chang Liu (e-mail: liuc@ohio.edu).}

\begin{abstract}
Avoiding objects independently for individuals with severe visual impairment is one of the significant challenges in daily life. This paper presents a wearable application to help visually impaired people quickly build situational awareness and traverse safely. The system utilizes Red, Green, Blue, and Depth (RGB-D) camera and an Inertial Measurement Unit (IMU) to detect objects and the collision-free path in real-time. A region proposal module is presented to decide where to identify the ground from 3D point clouds. The segmented ground area can act as the traversable path, and its corresponding region in the image is removed to prevent detecting painted objects. The system can provide information about the category, distance, and direction of the detected objects by fusing the depth image and the neural network results. A 3D acoustic feedback mechanism is designed to improve the situational awareness for visually impaired people, and guild them traverse safely. The advantage of this system is that our 3D region proposal module can robustly propose the potential ground region and greatly reduce the computation cost of the ground segmentation. Besides, a typical machine-learning-based approach may miss objects because they could not be recognized, though they still may pose a danger. Another advantage of our approach is that the imminent danger detector can detect such unrecognizable objects to help users avoid a collision. Finally, experiments by blindfolded subjects demonstrate that the proposed system provides a useful assistant tool to help blind individuals with collision avoidance and wayfinding.





\end{abstract}

\begin{keywords}
Convolutional neural network, ground segmentation, object detection, point cloud, region of interest, visual impairment, wearable assistive system.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
\PARstart{A}ccording to the World Health Organization (WHO) report\cite{IEEEexample:WHO}, there are no less than 2.2 billion people with visual impairment or blindness worldwide. Severely visually impaired individuals (i.e., those with best-corrected visual acuity worse than 20/200 in the best-seeing eye or those with less than 20 degrees of the functional visual field) face many challenges and risks in navigating their environments. These challenges include avoiding moving vehicles, a potentially life-threatening risk that has become an even more significant concern in recent years with the advent of near-silent hybrid and electric cars, further compounded by the increasing number of distracted drivers. These more modern challenges only add to the classic problems of spatial navigation and obstacle avoidance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{images/prototype1.png}
    \caption{The proposed system prototype.}
    \label{fig:prototype}%
\end{figure}

Traditionally, visually impaired people usually rely on navigational aids, namely guild dogs \cite{IEEEexample:BhatlawandeDesign} and canes \cite{IEEEexample:7879309} for help when detecting obstacles. However, they may still suffer injuries from hanging obstacles due to the limited detecting range of white canes and guide dogs. To improve mobility and safety for visually impaired people, advanced sensing techniques are utilized (e.g., ultrasonic sensors \cite{IEEEexample:shoval1998navbelt} \cite{IEEEexample:shaikh2018wearable}, visual sensors \cite{IEEEexample:brock2013supporting} \cite{IEEEexample:huang2015indoor}, and laser emitters \cite{IEEEexample:kulyukin2006robot}). However, ultrasonic sensors are poor at target identification or localization. Besides, laser emitters are expensive, heavy, and require high power consumption, which makes them unsuitable for wearable and affordable applications. Vision-based assistants, such as monocular cameras, stereoscopic cameras, and Red, Green, Blue, and Depth (RGB-D) cameras, have been widely used for target finding, localization, navigation, etc. For example, one important function of a vision-based application is to help visually impaired users find specific objects to use, such as chairs \cite{IEEEexample:takizawa2019kinect}, sign patterns in road environments, markers affixed to podiums and classroom doors \cite{IEEEexample:huang2019augmented}, up/down stairways \cite{IEEEexample:westfechtel20163d} \cite{IEEEexample:perez2014detection} \cite{IEEEexample:westfechtel2018robust} \cite{IEEEexample:souto2017stairs}, and elevators \cite{IEEEexample:kuramochi2014recognition}. For other applications, like visual simultaneous localization and mapping (VSLAM) for blind navigation has become a popular research field \cite{IEEEexample:murata2018smartphone} \cite{IEEEexample:bai2018virtual} \cite{IEEEexample:kacorri2018environmental} \cite{IEEEexample:liu2018augmented} as well. Because these applications assume object avoidance technology has been adopted, the visually impaired can reach their destinations independently and safely. Therefore, a wearable real-time object avoidance system is needed to assist the visually impaired in navigation and environment perception.


This paper proposes an object detection and avoidance system with region-based ground segmentation to improve mobility for visually impaired people.  Since RGB-D cameras are affordable and portable, they provide three-dimensional surrounding information directly without computational depth calculation needed when using only RGB cameras. Besides, Inertial Measurement Unit (IMU) delivers a means to measure acceleration and angular velocity through accelerometers and gyroscopes. The proposed system utilizes an RGB-D camera and IMU as the perceptual sensors to extract information about the surroundings, such as available free path, the type and direction of recognized obstacles. All calculations are completed locally in real-time. Hence no internet connection is needed, which might have privacy and access issues. The solution provides 3D sound feedback through the speakers to offer acute situational awareness and imminent danger alerts for the visually impaired. With a miniaturized design, as shown in Fig. \ref{fig:prototype}, the system can reduce the risk of catastrophic injury, thereby improving mobility and quality of life.

This main contributions of this paper are:
\begin{itemize}
    \item A wearable object detection and avoidance system can enhance the safe mobility of visually impaired people in real-time;
    
    \item A region-based ground segmentation can robustly segment the ground and reduce calculation time;
    
    \item Perform object detection both from point clouds and images to ensure accurate obstacle detection results;
    
    \item A 3D acoustic feedback optimized for different priorities to improve the situational awareness for blind people.

\end{itemize}




The rest of the paper is structured as follows. Section \ref{section:relatedwork} reviews various RGB-D based object recognition systems, ground segmentation approaches, and obstacle avoidance methods. Section \ref{section:framework} elaborates on the principle components of the proposed framework. After that, Section \ref{section:experiment} shows the experimental evaluations and discussions. Finally, the conclusions, along with future work, are discussed in the end.

\section{Related Work} \label{section:relatedwork}
The usage of robotic and autonomous driving techniques has been widely approached to aid safe navigation for visually impaired people. This research builds on important related work on the object recognition systems, the approach of ground segmentation, and the algorithmic solution to local obstacle detection.

\subsection{RGB-D based object recognition systems}
Since RGB-D based devices have advantages over other sensing devices as discussed above, in this section, we focus on the relevant research on the object recognition system with RGB-D sensors.   

Kayukawa et al. \cite{IEEEexample:kayukawa2019bbeep} developed a vision-sensing system, BBeep, for clearing the path in front of a blind user. This work was able to detect pedestrians and predict their future positions based on consecutive frames. By using 3D odometry API,  the system could remove the influence of the suitcase rotation in real-time. However, this system is not suitable to carry out for blind users because it was implemented based on a suitcase. 

Pradeep et al. \cite{IEEEexample:pradeep2010robot} proposed a wearable navigation system with obstacle detection and avoidance functionalities. Obstacles were detected by scanning 2D grids neighborhood quantized from the 3D point cloud. The system used haptic feedback to alert users when obstacles were detected in front. However, the system lacked obstacle modeling to recognize the category of objects. While object detection was only considered during the path planning phase, the system did not support real-time obstacle avoidance.

Li et al. \cite{IEEEexample:vision_based_mobile_indoor_nav_BingLi_2019} proposed an intelligent situational awareness and navigation aid, which was based on a Google Project Tangle device to help visually impaired people traverse indoor area independently. Dynamic planning is performed by real-time perception with an RGB-D camera. Based on a time-stamped occupancy map, real-time object detection was implemented through two 2D projections. This method has a lower cost but less information. For example, the type and shape of objects can not be determined. The detection of long-range ground and obstacles for visually impaired people was presented in \cite{IEEEexample:yang2018long}. Although the proposed system improved the pathfinding task for blind and visually impaired, it also did not provide any category information of the detected objects in the user’s surroundings.

 \begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{images/System_workflow.png}
    \caption{The workflow of the proposed system.}
    \label{fig:System_workflow}
\end{figure}

\begin{table*}
\caption{Comparison of different object detection and avoidance systems. $\checkmark$ := supported,$\times $ := unsupported, $\sim$:= unclear from the publication.}
\label{tab:publication comparison}
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\hline
 & Sensoring Devices  & Wearable &\begin{tabular}[c]{@{}c@{}}Real\\ Time \end{tabular}&\begin{tabular}[c]{@{}c@{}}Orientation \\ Tracking\end{tabular} &\begin{tabular}[c]{@{}c@{}}Height\\ Estimation \end{tabular}&\begin{tabular}[c]{@{}c@{}}Ground\\ Segmentation \end{tabular} & \begin{tabular}[c]{@{}c@{}}Object \\ Recognition\end{tabular}  & Feedback\\ \hline
Kayukawa et al. \cite{IEEEexample:kayukawa2019bbeep} &	RGB-D	&	$\times $	&	$\checkmark$		&$\times $&$\times $&$\times $&$\checkmark$& Acoustic	\\
Pradeep et al. \cite{IEEEexample:pradeep2010robot}  &  Stereo Camera $\&$ IMU &$\checkmark$&	$\checkmark$&$\checkmark$&$\checkmark$&$\times $& $\times $ & Acoustic\\
Li et al. \cite{IEEEexample:vision_based_mobile_indoor_nav_BingLi_2019}&RGB-D $\&$ IMU&$\checkmark$&	$\checkmark$ &$\checkmark$&$\checkmark$   &$\checkmark$ &$\times $& Acoustic\\
Yang et al. \cite{IEEEexample:yang2018long} &RGB-D $\&$ IMU &$\checkmark$&	$\checkmark$&$\checkmark$ &$\times $& $\checkmark$&$\times $&\begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular}  \\
Imai et al. \cite{IEEEexample:imai2017detecting} & RGB-D $\&$ Accelerometer&$\sim$&$\sim$&	$\checkmark$&$\checkmark$&$\checkmark$&  $\times $& $\times $\\
Takizawa et al. \cite{IEEEexample:takizawa2019kinect} &RGB-D &$\times$&$\sim$&$\times$& $\times$& $\times$ &$\checkmark$ &\begin{tabular}[c]{@{}c@{}}Acoustic\\ $\&$Haptic\end{tabular} \\ 
Bai et al. \cite{IEEEexample:bai2019wearable}&RGB-D $\&$ IMU&$\checkmark$ &$\checkmark$& \begin{tabular}[c]{@{}c@{}}Pitch-angle\\ Only \end{tabular} &$\times$ &$\checkmark$&$\checkmark$&Acoustic\\ 
Yang et al. \cite{IEEEexample:yang2016expanding}&RGB-D$\&$ IMU&$\checkmark$&$\sim$ &$\checkmark$&$\times$&$\checkmark$ &$\times$& \begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular} \\ 
Aladren et al. \cite{IEEEexample:aladren2014navigation} & RGB-D & $\checkmark$& $\sim$ & $\times$&$\times$ &$\checkmark$& $\times$&Acoustic\\ 
Lee et al. \cite{IEEEexample:lee2014wearable}&RGB-D $\&$ IMU&$\checkmark$& $\times$ &  $\checkmark$ &  $\checkmark$& $\checkmark$&  $\times$ &   Tactile              \\ 
Wang et al. \cite{IEEEexample:wang2017enabling}  &   RGB-D  & $\checkmark$&$\checkmark$&$\times$ &$\times$ &$\checkmark$& $\checkmark$&  \begin{tabular}[c]{@{}c@{}c@{}}Braille \\ $\&$Haptic \\ $\&$Acoustic\end{tabular} \\ 
Tapu et al. \cite{IEEEexample:tapu2017deep}   & Smart-phone  &$\checkmark$&$\checkmark$& $\times$ &$\times$ &$\times$&$\checkmark$& \begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular}\\
Ours                  &RGB-D $\&$ IMU & $\checkmark$ &  $\checkmark$& $\checkmark$&$\checkmark$ &$\checkmark$&$\checkmark$& Acoustic\\ \hline
\end{tabular}
\end{table*}


\subsection{Ground Segmentation from Point Clouds}

Ground segmentation is the first step in object detection because it aims to distinguish between viable ground areas and obstacles. Besides, visually impaired people could rely on the detected ground as the traversable road areas and search the optimal route \cite{IEEEexample:yang2018automatic}. Imai et al. \cite{IEEEexample:imai2017detecting} proposed an approach to identify the walkable path by using an RGB-D camera and an accelerometer. Their work estimated the initial orientation by decomposing the gravity on the tri-axis. Besides, the ground height and normal vectors were taken into consideration when detecting ground. However, the ground height calculation by one depth column is error-prone, if an obstacle happened to exist in there.

The Random Sample Consensus (RANSAC) is an iterative method and can be used for robustly estimating plane parameters from a set of data points. Takizawa et al. \cite{IEEEexample:takizawa2019kinect} used a RANSAC algorithm to extract wall and ground for helping blind people find an available seat. Bai et al. \cite{IEEEexample:bai2019wearable} applied RANSAC for coarse ground detection. Then a time-dependent adaptive ground height segmentation algorithm among adjacent frames was adopted to refine the ground result. In \cite{IEEEexample:yang2016expanding}, the traversable area preliminarily was obtained using RANSAC segmentation. Then, the preliminary traversable area was further enlarged by a growing region algorithm. However, these works treated the camera as static, and real-time orientation tracking was not considered. 

3D Hough transform (HT) for plane detection from point clouds were proposed in \cite{IEEEexample:hulik2014continuous}. HT describes every plane by its slope. The points in the parameter space will correspond to planes detected in the object space. After processing is finished, the accumulator in the parameter space will search the satisfying point, which corresponds to the shape of interest plane in the original space. Although HT is less accurate than the RANSAC, it needs less time to detect planes \cite{IEEEexample:zeineldin2016fast}.
 
 
There are other methods, like using graph descriptor, surface normal, or color information, to detect ground from the point cloud. Graph-based ground segmentation was proposed using a spatial mesh \cite{IEEEexample:strom2010graph}. Aladren et al. \cite{IEEEexample:aladren2014navigation} used both depth and color images to detect the ground. Although their system achieved high ground detection accuracy, it was too computationally expensive to use in real-time.  Holz et al. \cite{IEEEexample:holz2011real} calculated the local surface normal on the integral image. Then, these points were clustered and segmented in the normal space.




\subsection{Obstacle Avoidance Approaches}
Lee et al. \cite{IEEEexample:lee2014wearable} created a 2D probabilistic occupancy map to avoid obstacles in real-time during assistive navigation. The system built a 3D voxel map of the environment to analyze the traversability. Then, a 2D traversable grid map was used to support dynamic path planning. Point cloud clustering for Object detection were presented in \cite{IEEEexample:wang2017enabling} \cite{IEEEexample:vision_based_mobile_indoor_nav_BingLi_2019}. First, the detected ground was removed, and then the Euclidean segmentation was performed on the remaining points to cluster the objects. If the distance between two points is less than the threshold, then merge them. Finally, the algorithm's output is a set of object clusters, where each cluster is a collection of points of the same object. However, these methods are highly dependent on the size threshold of the merged objects and cannot provide category information to which the objects belong.



Object recognition based on machine learning methods has become increasingly popular. Tapu et al. \cite{IEEEexample:tapu2017deep} proposed a DEEP-SEE system to detect both dynamic and static objects using the You-Only-Look-Once (YOLO) object recognition method \cite{IEEEexample:redmon2016you}. The system was able to provide the obstacle category and distance information. However, this neural network needs heavy computation overhead, making it hard to fulfill real-time detection on mobile devices. Lightweight image-based neural networks have proposed to increase the accessibility of real-time object detection to mobile devices, such as MobileNet \cite{IEEEexample:howard2017mobilenets}, YOLO-LITE \cite{IEEEexample:DBLP:journals/corr/abs-1811-05588}, ShuffleNet \cite{IEEEexample:zhang2018shufflenet} etc. However, these lightweight algorithms do not consider distance information directly when performing object recognition. So, the painted object on the ground might be detected to mislead the visually impaired people.

To overcome the shortcoming of lightweight image-based algorithms, Bai et al. \cite{IEEEexample:bai2019wearable} proposed a 2.5-D object detection, which combined MobileNet object detection and depth image based object detection. The intersection between their mapped areas was used to improve the detection result. However, refinement by intersection may miss objects because they couldn't be recognized by a machine-learning-based approach even though they still may pose a danger. 

Table. \ref{tab:publication comparison} summarizes the functions and limitations of different assistive systems for visually impaired people. Compared with these works, our system is versatile. Not only can it dynamically track the orientation and altitude of the camera to segment the ground surface, but also provide real-time avoidance of obstacles and provide category information.

\section{The Proposed Framework} \label{section:framework}

As shown in Fig. \ref{fig:System_workflow}, the proposed system first obtains RGB and depth images from an RGB-D camera and gets acceleration and angular velocity from the IMU. Then, the real-time orientation and height of the camera are estimated by the enhanced depth image and IMU data. The system lifts RGB images and depth maps to 3D point clouds and reconstructs the actual 3D scene based on the orientation and height tracker. Then the potential ground area's region of interest (ROI) is proposed, thus the ground plane can be segmented effectively to search for available paths and  objects are detected over the ground. To further increase the stability, a feedback calibration for orientation and height estimation is presented.  The final objects with type, distance, and direction are calculated by fusing the output of the real-time CNN object detector and the corresponding depth image. To enhance system reliability, an imminent danger detector is implemented to capture the unrecognizable objects. The region corresponding to the ground area from the point cloud is removed from images before feeding into the CNN object detector, which reduces false-positive results such as painted objects on the ground. The system provides informative 3D acoustic feedback to users according to potential hazards and the feasible path that it has identified. The main algorithms of the proposed framework are described in detail in the following subsections.




\subsection{Depth Pre-processing}
The raw depth data output by the RGB-D camera may have "holes" and noise, which negatively affects system performance. Therefore, the pre-processing on depth is necessary to get better performance of the point cloud. This module comprises a decimation filter, a spatial filter, and a temporal filter to reduce data amount, fill holes, and improve noise while preserving features. 


Since the amount of data plays a crucial role in application performance, downsampling could reduce the density of points in very dense areas, but has little effect on sparse areas. Non-zero downsampling is performed by taking the average of neighbor pixels while ignoring zeros. The bilateral filter is used to smooth depth noise while preserving edges by calculating the one-dimensional exponential moving average (EMA). The recursive equation of EMA is represented as: 
\begin{equation}
 S_t=\left\{
\begin{array}{lc}
Y_1                                     &       t=1\\
\alpha Y_t +(1-\alpha)S_{t-1}           & t>1 \  and \  S_{t-1}- S_{t}\ <\ \delta \\
Y_t                                     & t>1 \ and\  S_{t-1}- S_{t}\ >\ \delta

\end{array} \right. 
\end{equation}
Where the coefficient $\alpha$ represents the degree of the weighting factor. $Y_t$ is the instantaneous value, $\delta$ is the threshold, and $S_t$ is the value of the EMA at time t.

% \begin{figure}[b]
%     \centering
%     \subfloat[]{{\includegraphics[width=0.4\linewidth]{images/rgb_Color.png} }}
%     \qquad
%     \subfloat[]{{\includegraphics[width=0.4\linewidth]{images/original_Depth.png} }}
%     \caption{An example of the raw data of the camera. (a) Color image. (b) Depth image with noises and black "holes".}
%     \label{fig:hole_in_depth_image}
% \end{figure}
\begin{table}[b]
\centering
\caption{Parameters of the depth pre-processing module.}
\label{tab:depth-preprocessing-parameters}
\begin{tabular}{|c|c|}
\hline
Filters    & Parameters                                                                                                               \\ \hline
Decimation & sub-sample = 5                                                                                                           \\ \hline
Spatial    & \begin{tabular}[c]{@{}c@{}}EMA: $\alpha = 0.25, \delta=20$\\ number of adjacent pixels for hole filling = 2\end{tabular} \\ \hline
Temporal   & number of frames for hole filling = 2                                                                                    \\ \hline
\end{tabular}
\end{table}

In a stereoscopic system, holes exist when either data being unavailable or not having met a confidence metric (e.g., occlusion, lack of texture .etc). In this situation, the camera sets the depth of these holes to zero instead of providing the wrong value. As shown in Fig. \ref{fig:depth-pre-processing} (b), the holes are displayed as black dots. The method of filling holes is a combination of spatial and temporal filters. The holes are filled first with valid adjacent pixels within the specified radius in the same frame. Besides, if holes still occur, they are filled with the last valid (if they exist) value by looking at several frames back in history.  
\begin{figure}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/rgb_Color.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/original_Depth.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/pre_process_Depth.png} }}%
    \caption{Results of depth image enhancement. (a) Raw color image. (b) Raw depth image with noises and black "holes". (c) The enhanced depth image after pre-processing.}%
    \label{fig:depth-pre-processing}
\end{figure}

Based on the recommendation from \cite{IEEEexample:grunnet2018depth}, the RealSense D435i camera is streaming depth data at 840 by 480 resolution and with the infrared sensor on to have the best quality of the depth image. In the depth pre-processing module, the parameters of filters are listed in Table. \ref{tab:depth-preprocessing-parameters}. The raw depth image is shown in Fig. \ref{fig:depth-pre-processing} (b), which has noises and holes. After processing by decimation filter, spatial filter, and temporal filter, the quality of the depth images is improved with few or no holes, while preserving features, shown in Fig. \ref{fig:depth-pre-processing} (c).

\subsection{Ground region proposal}
Since the point cloud has the feature of 3D Euclidean measurements in front of the camera, it is essential to calculate the relative position between the camera and the object to understand the absolute position of objects. We provide real-time orientation and altitude tracker to help calculate the relative position. In our implementation, we only consider the pitch and roll angles. Then, the scene is reconstructed based on the relative position. Besides, the potential ground area is suggested based on the height of the plane in the reconstructed 3D scene.

 
\subsubsection{Orientation and height tracker} \label{section:orientation tracker}

 \begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]  {images/InitialOrientation.png}
    \caption{Definition of Euler angles. Left: the camera with zero degrees of Euler angles; Middle: the pitch angle, which is respect to Z-axis while clock-rise rotating around X-axis; Right: the roll angle is regarding X-axis while clock-rise rotating around Z-axis.}
    \label{fig:InitialOrientation}
\end{figure}

Orientation and height estimation within the world coordinate space play a principal role in proposing the region of interest. As shown in Fig. \ref{fig:InitialOrientation}, the camera is centered at a right-handed Cartesian coordinate system, where the positive X, Y, and Z-axis are defined as the cameras left direction, up direction, and facing direction,  respectively. We use $A_{X,OUT}$ as the acceleration along the X-axis, subsequently $A_{Y,OUT} $, and $A_{Z,OUT} $ are accelerations in Y and Z axes. The initial orientation of the camera is calculated by decomposing gravity g from three-axis accelerations, which is represented as:  

\begin{equation}
Pitch = tan^{-1}(\frac{\sqrt{A^2_{X,OUT}+A^2_{Y,OUT}}}{A_{Z,OUT}})
\label{equation: pitch}
\end{equation}

\begin{equation}
Roll = tan^{-1}(\frac{A_{X,OUT}}{\sqrt{A^2_{Y,OUT}+A^2_{Z,OUT}}})
\label{equation: roll}
\end{equation}




In theory, the real-time orientation can be gauged through integrating the output of the gyroscope. However, the estimation results suffer from the integration of drift over time. To help fix the time drift of the gyroscope, we use the complementary filter to allow short-duration signals from the gyroscope to pass through while blocking steady signals. The noise and horizontal acceleration dependency are also alleviated by the complementary filter, which is formulated as:
\begin{equation}
Angle(t) = \alpha  (Angle(t) + \int{gyro(t)}  dt) + (1-\alpha)  acc(t)
\label{equation: Complementary Filter}
\end{equation}
Where $\alpha$ is the filtering weight, $gyro(t)$ is the output of the gyroscope at time $t$, and $acc(t)$ is the output of the accelerometer at time $t$.

The only knowledge about the camera pose is the approximate position of the camera on the chest because the subject ’s height and movement are continually changing. The initial camera height estimation is equivalent to calculating the distance from the camera position to the ground. The RANSAC program is executed in the initial system setup to search the ground. During the system setup, the camera is facing down statically, and an additional pass filter on the y-axis can be used to limit the search of the ground plane. Repeat the process until the floor is detected. Since the user should stand on the floor, this process will not last long.

Once the ground plane is determined, it can be represented as \eqref {equation: plane model}, where A, B, C, and D are the coefficients of the ground plane. From the point-plane distance, the height of the camera $H_{camera}$ can be computed as \eqref{equ:height}. Besides, due to the bias and noise presented in IMU, the normal vector $n$ (e.i., $(A, B, C)$) is used to calibrate the estimated initial pitch angle. Real-time altitude estimation is implemented using feedback calibration, which will be explained in detail later.
 \begin{equation}
Ax + By + Cz + D = 0 
\label{equation: plane model}
\end{equation}

\begin{equation}
H_{camera}=\frac{D}{\sqrt{A^2+B^2+C^2}}
\label{equ:height}
\end{equation}


\subsubsection{3D reconstruction} 
As long as the orientation and the height of the camera are estimated, the 3D scene can be reconstructed. The rotation angles of interest are pitch angle ($\theta$) and roll angle ($\alpha$) corresponding to the x-axis and the z-axis. The rigid transformation from the camera coordinate system to the world coordinate system is achieved by a combination of a rotation matrix $\textbf{R}$ and a transition vector $\textbf{t}$.  The rotation matrix is used to make the normal vector $n$ parallel to $(0, 1, 0)$, and the transition matrix is used to move a point to the origin. The affine transformation matrix $\textbf{T}$ in the homogeneous coordinate is represented as:



\begin{equation}
\begin{aligned}
\textbf{T} &=
\begin{bmatrix}
  \textbf{R} & \textbf{t}\\ 
  \textbf{0}^T & 1
\end{bmatrix}
= 
\begin{bmatrix}
  \textbf{R}_z\textbf{R}_x & \textbf{t}\\ 
  \textbf{0}^T & 1
\end{bmatrix} 
\\
&=
\begin{bmatrix}
  \cos{\theta} & -\sin{\theta}\cos{\alpha} & \sin{\theta}\sin{\alpha} & 0\\ 
  \sin{\theta} & \cos{\theta}\cos{\alpha} & -\cos{\theta}\sin{\alpha} & -H_{camera} \\
  0 & \sin{\alpha} &\cos{\alpha}  & 0 \\
  0 & 0 & 0 & 1
\end{bmatrix}
\label{Equ:transform}
\end{aligned}
\end{equation}

where: 
\begin{equation}
\begin{aligned}
R_z&=
\begin{bmatrix}
   \cos{\theta} & -\sin{\theta} & 0 \\ 
  \sin{\theta} & \cos{\theta}   & 0\\
  0 & 0 &1
\end{bmatrix} \\
R_x&=
\begin{bmatrix}
   1& 0 & 0 \\ 
  0 & \cos{\alpha}   & -\sin{\alpha}\\
  0 & \sin{\alpha} & \cos{\alpha}
\end{bmatrix} \\
 \textbf{t} &= 
\begin{bmatrix}
 0\\ 
 -H_{camera}  \\
 0 
\end{bmatrix}
\label{Equ:roatation}
\end{aligned}
\end{equation}
After the original point cloud is reconstructed based on the real-time orientation and height tracker, the absolute position of objects in the scene is obtained. Since the ground has the following characteristics: parallel to the XZ plane and the intersection point on the y-axis is 0, a band-pass filter is used to propose a ground ROI along the y-axis, for example, from -0.1 meters to 0.1 meters. The advantage of searching the ground in the ROI is that it dramatically reduces the 3D search space for the RANSAC algorithm and saves the calculation time.

\subsection{Ground Segmentation}
Ground segmentation is essential to understand the surrounding environment because the segmented ground can be severed as the three purposes: 

\begin{itemize}
\item Wayfinding for users, use the segmented ground area as the free movable area;
\item The parameters of the ground plane can be used to calibrate the orientation and height tracker;
\item Applying object detection over the segmented ground. 
\end{itemize} 

\subsubsection{Ground segmentation}


To segment the ground, we adopt the guaranteed algorithm RANSAC to the ROI out of the region proposal module. The RANSAC algorithm starts from randomly choosing a set of 3D points to estimate the parameters A, B, C, and D in  \eqref{equation: plane model}. The remaining points are evaluated through \eqref{equ:inliers} to count the number of inliers (T is the distance threshold). After certain iterations, the ground plane is determined, which has the most inlier points. The number of iterations is formulated as k, as shown in \eqref{equ:RANSAC repeatation}.

\begin{equation}
\frac{|Ax+By+Cz+D|}{\sqrt{A^2+B^2+C^2}}<T
\label{equ:inliers}
\end{equation}



\begin{equation}
k=\frac{\log(1-p)}{\log(1-w^n)} 
\label{equ:RANSAC repeatation}
\end{equation}
Where $1-w_n$ is the probability that at least one of the n points is failing the computation of outliers, $1-p$ is the probability that is independently selecting a set of n points are not all inliers. 

In the sampling stage, some sampling errors are eliminated by setting the tilt angle threshold (as in \eqref{formula:inclination angle}) of the ground plane. The perpendicular plane model is adopted to restrict the wall-like vertical plane by checking the normal direction. Even though the proposed ROI restricts the vertical height range to eliminate the error-prone planes (e.g., table surfaces), the distance of the detected plane from the origin is checked to make detection result more reliable. 


\begin{equation}
\theta=\arccos{\frac{|B|}{\sqrt{A^2+B^2+C^2}}}
\label{formula:inclination angle}
\end{equation}

\subsubsection{Feedback calibration}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/CNNfalse.png}
    \caption{The "2D" object detector treats an image on a piece of newspaper on the ground as a dangerous obstacle.}
    \label{fig:cnn-false}
\end{figure}

As mentioned in section \ref{section:orientation tracker}, the real-time height estimation is performed after the coefficients of the ground plane were determined. The height is calibrated during movement by updating the transform matrix using the intersection D of the plane. At the same time, we also update the transform matrix to ensure the normal vector n of the ground is parallel to (0,1,0). Therefore, the long-term drift of the gyroscope is further reduced. The closed-loop feedback mechanism makes the system robust when used for long periods under walking conditions.
\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{images/objectdetection.png}
    \caption{The procedure of the CNN-based object detection.}
    \label{fig:objectdetection_procedure}
\end{figure}

\begin{comment}
\subsubsection{Ground segmentation}
Although RANSAC is a guaranteed algorithm, the number of inliners depends on the number of iterations it runs. A seeded region growing algorithm is applied to save computation time and expand the traversable area longer and broader. The calculation of the normal vector of the point cloud is solved using the Principal Component Analysis. For each point and K neighborhood groups, the third component obtained from the analysis corresponds to the normal direction, i.e., flipped towards the viewpoint. Then, the growth starts from the seed pixel, which is chosen according to the preliminary RANSAC-based ground detection. The growing process is shown in Algorithm \ref{alg:regiongrowing}. Finally, the short-range ground plane has been enlarged to a longer and broader traversable area.




\begin{algorithm}
\caption{A seeded region growing algorithm.}
\label{alg:regiongrowing}
\LinesNumbered
\KwIn{\\
$c$: input point cloud \\
$seed$: a point where the growth starts \\
$n$: number of neighbors \\
$c_{th}$: curvature threshold\\
$\theta_{th}$: angle threshold}     
\KwOut{\\
$G$: the point list of the ground segmentation} 
\textbf{procedure} RegionGrowing($c, n, seed, c_{th}, theta_{th}$) \\
\Indp
\textbf{initialization:}  \\
\quad $kd\_tree(c), G\leftarrow \emptyset $\\
\quad ${S} \leftarrow \emptyset$ \Comment*[]{Current seeds list}
$N=normalEstimate(c)$\\
$C=curvatureEstimate(c)$ \\
append $seed$ to ${S}$ \\
\While {${S}$ is not empty}{
	retrieve a seed $s$ from the list $S$ \\
		using $Kd\_tree$ to find nearest n neighbors points $P$ of $s$  in $c$ \\
		\For{point $p$ in $P$}{
			\If{$p$ is not visited and $\cos^{-1}(|N\{s\},N\{p\}|)<\theta_{th}$}{
				append $p$ to $G$ \\
				\If{$C\{p\}<c_{th}$}{
					append $p$ to ${S}$}
			}
		}
		remove $s$ from $S$
	
}
\textbf{return} $G$
\end{algorithm}

\end{comment}



\subsection{Object Detection}
To make the system reliable, we presented two ways to detect objects:  the CNN based object detection and the imminent danger detection. 


\subsubsection{CNN based object recognition}

Based on the investigation of the trade-off between speed and accuracy for modern convolutional object detectors \cite{IEEEexample:huang2017speed}, we choose the combined detector, SSD\_MobileNet\_V2, as the object recognizer running on the mobile device.
By using depthwise separable convolutions to reduce both numbers of parameters and computational cost, the MobileNet\-V2
\cite{IEEEexample:sandler2018mobilenetv2} archives a better balance between lightweight and performance than other networks \cite{IEEEexample:ren2015faster} \cite{IEEEexample:liu2016ssd} \cite{IEEEexample:howard2017mobilenets} \cite{IEEEexample:zhang2018shufflenet}. The detector uses MobileNet\-V2 as the feature extractor from images, then generates anchors using SSD \cite{IEEEexample:liu2016ssd}. The detector is trained on the Common Objects in Context (COCO) dataset\footnote{\url{http://cocodataset.org/}} from Microsoft, which includes 91 classes (e.g., person, chair, table), to help visually impaired people have a general perception of surroundings. However, the "2D" object detector cannot provide the distance information for blind navigation. 



Therefore, the result of the CNN object detector is fused with the depth map to get the distance information. The distance of the detected object is computed by averaging the value in the corresponding depth map area. Thus, a rich information about surrounding objects is provided, such as the category and the distance. Because the depth is not considered in advance, a painted object on the ground might still be detected as an obstacle, which will lead to visually impaired people making wrong decisions. (see Fig. \ref{fig:cnn-false}). In order to eliminate this false positive detection result, the ground area identified from the point cloud is mapped to the original image according to the transformation matrix, and the corresponding ground area is masked from the image. The procedure is shown in Fig. \ref{fig:objectdetection_procedure}.





\subsubsection{Imminent danger detection}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/FoV.png}
    \caption{Narrowed horizontal FoV search area.}
    \label{fig:Fov}
\end{figure}



The CNN based object detector is not able to recognize some objects, and they may pose a danger to the user.
Our system features an imminent danger (e.g., hanging objects) detection that can detect any potentially dangerous obstacles and output their 3D bounding box.  The imminent danger detection is performed using Euclidean clustering on the 3D point cloud. Rather than utilizing whole point cloud as input, the points belong to the ground area are removed. Besides, a passthrough filter is performed to focus on specific areas. For example,the area within 1 meter of the user is considered, because obstacles in this range may pose a danger to the user. In addition, when the distance from the user is less than 0.5 meters, the detection area is further reduced in the horizontal field of view (FoV) (Fig. \ref{fig:Fov}). This strategy allows the algorithm to focus on the user's straight direction to reduce false alarms, such as the door frame might is detected as an obstacle when crossing the door. 
\begin{algorithm}
\caption{Hazard detection based on Euclidean clustering approach.}
\label{alg:EcludeanClustering}
% \LinesNumbered
\KwIn{\\
$c$: input point cloud \\
$min$: the minimum size of the cluster \\
$max$: the maximum size of the cluster\\
$t$: the search radium}     
\KwOut{\\
$obL$: obstacle bounding box list[$x\_min, y\_min, z\_min, x\_max, y\_max, z\_max$]} 
\textbf{procedure} HazardDetection($c, min, max, t$) \\
\Indp
\textbf{initialization:}  \\
\quad $kd\_tree(c), C \leftarrow \emptyset, Q\leftarrow \emptyset$ \\
\For{point $i$ in $c$}{
	\If{$i$ is not visited}{
		append $i$ to $Q$} 
	\For{point $q$ in $Q$}{
		using $Kd\_tree$ to search neighbor points $P$ of $q$ within the radium of $t$ in $c$ \\
		\For{point $p$ in $P$}{
			\If{$p$ is not visited}{
				append $p$ to $Q$ 
			}
		}
	}
	\If{the size of $Q$ is between $min$ and $max$}{
		append $Q$ to $C$
	}
	reset $Q$
}
\For{$cluster$ in $C$}{
	 $ob$ $\leftarrow$ bounding$\_$box($cluster$) \\
	append $ob$ to $obL$
}
\textbf{return} $obL$
\end{algorithm}


The k-dimensional (three-dimensional in our example) tree is used for range search and nearest neighbor search. Euclidean cluster extraction divides the input point cloud into a cluster list, which represents the detected obstacles. As described in Algorithm \ref{alg:EcludeanClustering}, the output of a set of 3D bonding boxes illustrate the position and size of the identified obstacles.




\subsection{Acoustic Feedback}
As described in related work \cite{IEEEexample:kayukawa2019bbeep} \cite{IEEEexample:edworthy1991improving}, the human reaction of audible emergency warnings has been investigated. We had a series of people who are actually blind. They were the ones who suggested feedback abilities. This call occurred and that many of the goals were shaped on the potential end-user feedback. We also conducted studies on eye-folded people to design a sound-emission strategy for our system. The sound emission policy includes three stages of alert to prevent collisions and annoying, as presented in Table. \ref{tab:acoustic pattern}.

\begin{figure}
    \centering
    \subfloat[]{{\includegraphics[width=0.45\linewidth]{images/urgent_rgb.png} }}%
    \subfloat[]{{\includegraphics[width=0.45\linewidth]{images/urgent_depth.png} }}%
    \caption{An example of the imminent danger detection. (a) A hanging object is very close to the user's front. (b) The imminent danger detector can detect this potential danger from the point cloud, even if CNN did not recognize it.}%
    \label{fig:urgent_danger_detection}
\end{figure}




The low emergency sound warns that a collision may occur between 0.3 and 1 meters. The beep sound with low tone is spatialized in the 3D audio environment to guild visually impaired people. From the 3D reconstruction module, the user's position and orientation in the world coordinate were acquired. The output from the speakers is determined by the relative position of the detected object to the user. One example is illustrated in Fig \ref{fig:3Daudio}. The user is oriented along the positive Z-axis, and his head is pointing towards up. If the position of the detected object on the X-axis has a positive value, the sound emitted from the location of the object will be heard from the left speaker. The volume of the sound emitted is controlled by attenuation (a multiplicative factor). The higher the attenuation, the less sound the user hears. If a warning occurs, the user should slow down the walking speed to continue.

\begin{table}[t]
\centering
\caption{Acoustic feedback patterns.}
\label{tab:acoustic pattern}
\begin{tabular}{|c|c|c|c|}
\hline
Level   & Type  & Pattern                & Time \\ \hline
prompt  & human & "1 chair 3 meters ahead" & 5s   \\ \hline
warning & beep  & 3D audio with low-key             & 1s   \\ \hline
urgent  & human & "no path found"          & 3s   \\ \hline
urgent  & beep  & high-key              & 1s   \\ \hline
\end{tabular}
\end{table} 

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/3Daudio.png}
    \caption{3D audio feedback.}
    \label{fig:3Daudio}
\end{figure}
If the detected obstacle is within 0.3 meters or no available path is identified, the system will continue to issue an alarm to notify the user. In this case, the user should stop and then turn left / right to search for the best walking direction until the warning stops. In addition to providing warnings, voice prompts are used for users to improve their situational awareness. For example, if the user enters an unfamiliar environment, the user can activate the system to convert the result of the CNN object detector into speech to describe the scene to the user.



\section{Experimental Results and Discussions} \label{section:experiment}

 Experiments were conducted to evaluate the performance of the main modules under real scenarios. The whole system was pilot tested with a single blindfolded subject for its quantitative and qualitative effectiveness in guiding users to move around safely. The test scenario was in the author's home due to the COVID-19 stay-at-home order in Ohio.


% \subsection{Experiments on Depth Pre-processing}
 



\subsection{Experiments on ground segmentation}
\subsubsection{ Orientation Tracking}

\begin{figure}
    \centering
    \subfloat[]{{\includegraphics[width=0.49\linewidth]{images/pitch.png} }}%
    \subfloat[]{{\includegraphics[width=0.48\linewidth]{images/Roll.png} }}%
    \caption{The estimation of pitch and roll angles. Note: (a) is the pitch angle measurement; (b) is the roll angle measurement; $\alpha = 0.98$ for the complementary filter.}%
    \label{fig:anglestimation}
\end{figure}
Sensitivity, zero-offset, and bias are the intrinsic parameters of IMU, and they attempt to convert the raw sensor data into real measurements. These intrinsic parameters were calibrated to improve the accuracy of the angle estimation. First, IMU data in 6 different static positions (upright outward, upward and downward, etc.) were collected. We calibrated these parameters to ensure the accelerometer's output is consistent with the gravity decomposition on three axes. Then, the quantitative analyses of the real-time orientation tracker were performed. 

We evaluated the orientation tracker in a pre-set direction and compared the estimation results with the ground truth in three cases: using only the gyroscope, using complementary filters on the gyroscope and accelerometer, and applying feedback calibration to the complementary filters. The comparisons of pitch angle estimation and roll angle estimation are shown in Fig. \ref{fig:anglestimation}. The gyroscope based estimations could not be used in practice because of the time drift. The complementary filters illustrate good results; however, mechanical devices inevitably have biases and noises. Our feedback calibration expresses excellent results, because similar to the Kalman filter's mechanism, another discrete resource is invoked to correct bias. 


\subsubsection{Ground Segmentation}

\begin{table}[t]
\centering
\caption{Parameters of RANSAC algorithm for ground segmentation.}
\label{tab:ground-segmentation-parameters}
\begin{tabular}{|c|c|}
\hline
Model Type        &  NORMAL PLANE    \\ \hline
Max iterations       &  200    \\ \hline
Input normal        &  True    \\ \hline
Distance threshold        &  0.1m    \\ \hline
Axis       &  (0, 1, 0)    \\ \hline
Angle threshold        &  $5^\circ$    \\ \hline
Max distance from origin        &  0.1m    \\ \hline
\end{tabular}
\end{table}


  As depicted in Fig. \ref{fig:groundsegmentation}, three situations in the test scene were tested to evaluate ground segmentation's performance. The first case is on the left, with no obstacles on the floor. The ground had been successfully segmented, but several spots were not included. We adjusted parameters using loose constraints of the distance and angle. The missing spots can be segmented, but the whole ground area invaded to the object surface adjacent to the ground.  Since the floor near the door in the test scene is not flat, and the in distance area does not affect the whole system performance. We set the angle threshold to 5 degrees and the distance threshold to 0.1 meters to get balance results. The parameters of the RANSAC algorithm are listed in Table. \ref{tab:ground-segmentation-parameters}. These parameters are selected according to the performance of the segmentation, that is, including as many true-positive points as possible and few false-positive points. Then, scenes with obstacles in the distance and nearby were evaluated. We can see that the proposed algorithm has robust segmentation results in these cases. 

\begin{figure}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/openspace.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/normalspace.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/crowedspace.png} }}%
    \caption{Results of the ground segmentation. (a) Open space without obstacles. (b) Space with obstacles in the distance. (c) Space with an obstacle nearby. Note: The blue region represents the segmented ground.}%
    \label{fig:groundsegmentation}
\end{figure}

\subsection{Experiments on Object Detection}
Three scenarios were tested in an indoor environment to verify our object detection method. For the CNN object detection model, the confidence threshold was set to 0.5 to filter out low-confidence results. The distance threshold for the imminent danger detection was set to 0.2 meters to separate point clusters.


As shown in Fig. \ref{fig:objectdetection}, we first tested the ability to detect large and small everyday objects. In case 1 and 2, imminent danger detection from point clouds and CNN-based object detection from images can successfully identify the bottle and the chair.
For exceptional case, medium-sized unusual object was tested. The CNN model could not identify object because there was no category with a confidence score higher than 0.5. However, our imminent danger detection method could still detect the missing object. Regardless of the size and type of object, the imminent danger detection is a reliable method. It can identify objects that are missing from machine learning-based methods and ensure that the object detection can be performed reliably by blind people.







% to test whether object detection can help visually impaired people travel more effectively. According to the urgent detection results in our system, users can hear 3D low-key beeps from two speakers according to the sound emission pattern. However, urgent detection does not provide category information about blocked objects. The users must turn left or right to search for the direction of movement. If users activate CNN-based object detection, the human speech will describe more information about the surrounding environment. In this case, the users know the object and can push the chair away to prevent them from taking more detours and improve their environmental awareness.

\begin{figure}
    \centering
    \captionsetup[subfigure]{labelformat=empty}
    \subfloat[case 1(a)]{{\includegraphics[width=0.32\linewidth]{images/originalbottle.png} }}%
    \subfloat[case 1(b)]{{\includegraphics[width=0.32\linewidth]{images/bottle.png} }}%
    \subfloat[case 1(c)]{{\includegraphics[width=0.32\linewidth]{images/CNNbottle.png} }}%
    \qquad
    \subfloat[case 2(a)]{{\includegraphics[width=0.32\linewidth]{images/original.png} }}%
    \subfloat[case 2(b)]{{\includegraphics[width=0.32\linewidth]{images/chairdep.png} }}%
    \subfloat[case 2(c)]{{\includegraphics[width=0.32\linewidth]{images/CNN_result.png} }}%
    \qquad
    \subfloat[case 3(a)]{{\includegraphics[width=0.32\linewidth]{images/resultcan.png} }}%
    \subfloat[case 3(b)]{{\includegraphics[width=0.32\linewidth]{images/trashcan.png} }}%
    \subfloat[case 3(c)]{{\includegraphics[width=0.32\linewidth]{images/CNNcan.png} }}%
    \caption{Results of the object detection. Case 1: Small object (a bottle) detection; Case 2: Large object (a chair) detection; Case 3: Unrecognized object by the machine learning model. Note: (a)s are RGB images; (b)s are results of the imminent danger detection from point clouds, the direction is left and right  reversed relative to RGB images; (c)s are results of the CNN-based object detection from images; The green regions represent the segmented ground from point clouds; The blue regions represent the segmented ground in images.}%
    \label{fig:objectdetection}
\end{figure}

\subsection{Computational Cost}

The hardware includes an Intel RealSense D435i camera\footnote{https://www.intelrealsense.com/depth-camera-d435i/}, a Raspberry Pi model b with 4Gb memory\footnote{http://www.raspberrypi.org/products/raspberry-pi-4-model-b/}, and an ANKER PowerCore 10000 mobile power supply\footnote{http://www.anker.com/products/variant/powercore-10000-pd/A1235011}. Point Cloud Library (PCL) version 1.9.0 is used to process point clouds, and Open Multi-Processing (OpenMP) is applied to achieve multi-threaded parallelism. The machine learning model for object detection is optimized using quantization to reduce the reference time. In the sound feedback module, the Flite text-to-speech engine\footnote{http://www.festvox.org/flite/} and the Open AL audio engine\footnote{https://www.openal.org/} are adopted to provide human speech and 3D audio feedback.

All algorithms run on the local Raspberry Pi. The point clouds are downsampled to 160 by 120, and images are cropped to 300 by 300. As depicted in Table. \ref{tab:computation cost}, the average computational time among 100 epochs for the three cases in Fig. \ref{fig:objectdetection} were calculated. We show that the time spent on ground segmentation and imminent danger detection has a positive correlation with the size of the object. Compared with case 1, case 2 requires more than 26\% of the time to segment the ground. Because the smaller the percentage of the ground area in the regional proposal, the more time it takes to segment it from random sampling. Besides, due to the need to cluster more points for larger objects, case 2 takes 2.03 milliseconds, 3.44 times longer than case 1. Since CNN-based object detection is only activated when a visually impaired user wishes to understand the surrounding environment, the total time of all algorithms is about 358 milliseconds on average. Therefore, the proposed system can provide real-time assistance for typical walking situations.

\begin{table}[]
\centering
\caption{Average running time among 100 epochs. Note: The total and average time are calculated with CNN-based object detection excluded.}
\label{tab:computation cost}
\begin{tabular}{|c|c|c|c|}
\hline
Stage                      & Case 1  & Case 2 & Case 3 \\ \hline
Depth preprocessing        &    114.34 ms  &  118.32 ms & 120.70 ms\\ \hline
3D scene reconstruction    &   61.57 ms   & 61.74 ms & 60.28 ms\\ \hline
Normal estimation          &    37.88 ms& 39.92 ms &  39.87 ms \\ \hline
Ground segmentation            &   122.91 ms  & 155.11 ms & 148.57 ms \\ \hline
Immient danger detection &     0.59 ms&  2.03 ms &  1.06 ms \\ \hline
CNN-based object detection       &   65.89 ms  &  70.21 ms & 70.52 ms\\ \hline
Total & 337.29 ms& 365.12 ms & 370.48 ms\\ \hline
Average &\multicolumn{3}{c|}{357.68 ms}  \\ \hline
\end{tabular}
\end{table}



\subsection{Discussions}

Through quantitative and qualitative evaluations, our wearable system effectively guides visually impaired people to avoid obstacles in daily life using low-power embedded devices. 
Different from works \cite{IEEEexample:takizawa2019kinect} \cite{IEEEexample:yang2016expanding} that segment ground in the whole search space, or search the area below the estimated ground height \cite{IEEEexample:bai2019wearable}, we propose a novel architecture that can segment the ground on the proposed region based on real-time orientation and altitude tracker. This method reliably proposes the ground ROI and greatly reduces the 3D search space of the RANSAC algorithm. The performance improvement is highly dependent on the concrete scenes because of the irregularity of the point cloud. We compared the ground segmentation time in the whole space and the proposed region of the three typical indoor scenes in Fig.\ref{fig:groundsegmentation}. In scenario (a), the average time of region-based segmentation is shortened by an average of 73.73\%, in scenario (b) by 50.85\%, and in scenario (c) by 57.78\%. The results are depicted in Fig. \ref{fig:comparison}, which demonstrates the efficiency of our method on ground segmentation using a low power device.

The object detection from the point cloud is a guaranteed method that can reliably detect any potential hazards. Still, it is not efficient to identify the category of objects through modeling. Machine-learning-based methods can provide the category information about detected objects, but they may miss objects, although they may still pose a danger. Another advantage of our approach is that object detection is performed from point clouds and images. Such unrecognized objects can be detected by the imminent  danger detector to help users avoid collisions. Simultaneously, machine learning methods can be used to improve the situational awareness of blind people. Since machine learning is 
a data-driven model that predicts objects based on the trained data, we can retrain the machine learning model by capturing images of the target environment to improve the accuracy. 

Instead of sensing the reflected sound from the parametric speaker \cite{IEEEexample:takizawa2019kinect} to help users understand the angle of the object, we use 3D audio feedback to provide not only the direction but also the distance information of the object. Besides, if the object's surface is not flat, the parametric speaker may not provide the correct guidance. Our system can help build a more accurate understanding of the surrounding environment by offering objects with a sense of distance and direction to avoid obstacles.

The system runs on the Raspberry Pi at about 3 Hz in real time. By overclocking the CPU frequency to a maximum of 2 GHz, the operating speed is slightly increased. By further downsampling the point cloud to a smaller size, the processing time can be reduced, which is a compromise between detection accuracy and speed. We also tested the system on a desktop computer with an Intel 8700K CPU and 16 Gb RAM, and the results showed an approximately 10-fold increase in update speed. Although the hardware prototype is less powerful, it consumes less power and can still provide timely feedback.
\begin{figure}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/SegCompare1.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/SegCompare2.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/SegCompare3.png} }}%
    \caption{Comparison of time spent on ground segmentation among 100 epochs (including normal estimation). (a) Testing on the scenario Fig \ref{fig:groundsegmentation}(a). (b) Testing on the scenario Fig \ref{fig:groundsegmentation}(b). (c) Testing on the scenario Fig \ref{fig:groundsegmentation}(c).}%
    \label{fig:comparison}
\end{figure}

Our method still has limitations though. The neural network is only trained for common objects. For special objects, it cannot provide the correct semantic category information for the end-user, for example, case 3 in Fig. \ref{fig:objectdetection}. Since we do not search the space under the ground in the point cloud, the imminent danger detector is almost impossible to detect down-stairs. The acoustic feedback may distract blind people because the sound is the main source of blind people's perception of the environment.


\section{Conclusions and Future Work} \label{section:conclusion}
This article proposes a wearable imminent danger detection system that can assist people with severe visual impairments to move around independently. The system features include wayfinding, imminent danger detection, object recognition, and 3D acoustic feedback. The user's surroundings can be reconstructed accurately by tracking the orientation and altitude of the camera. Thus, we proposed a region-based ground segmentation, which reduces the computational cost and increases robustness. The imminent danger detector provides a guaranteed method for identifying obstacles that are unrecognizable by the machine learning method but still might pose a danger. By utilizing an optimized 3D audio feedback mechanism based on the detected potential threats, our system can help users prevent imminent hazards and improve their situational awareness.

 
We conducted thorough experimental evaluations in common indoor environments. Experimental results prove that our effective technology can be a useful tool to help blind people reduce travel injuries. In the future, we plan to apply deep neural network models directly on the 3D point cloud to estimate object-oriented 3D bounding boxes and semantic categories to improve performance further. Besides, adding a tactile wrap on the wrist to improve the feedback mechanism. 


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,IEEEexample}

%\begin{thebibliography}{10}
%
%\bibitem{WHO}
%{World Health Organization}.
%\newblock Blindness and vision impairment.
%\newblock
%  \url{https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment},
%  2019.
%\newblock Online; accessed 29 April 2020.
%
%\bibitem{BhatlawandeDesign}
%Shripad Bhatlawande, Manjunatha Mahadevappa, Jayanta Mukherjee, Mukul Biswas,
%  Debabrata Das, and Somedeb Gupta.
%\newblock Design, development, and clinical evaluation of the electronic
%  mobility cane for vision rehabilitation.
%\newblock {\em IEEE Transactions on Neural Systems and Rehabilitation
%  Engineering}, 22, 05 2014.
%
%\bibitem{7879309}
%H.~{Zhang} and C.~{Ye}.
%\newblock An indoor wayfinding system based on geometric features aided graph
%  slam for the visually impaired.
%\newblock {\em IEEE Transactions on Neural Systems and Rehabilitation
%  Engineering}, 25(9):1592--1604, Sep. 2017.
%  
% \bibitem{shoval1998navbelt}
%Shraga Shoval, Johann Borenstein, and Yoram Koren.
%\newblock The navbelt-a computerized travel aid for the blind based on mobile
%  robotics technology.
%\newblock {\em IEEE Transactions on Biomedical Engineering}, 45(11):1376--1386,
%  1998.
%  
%\bibitem{shaikh2018wearable}
%Farooq Shaikh, Mohammad~Abbas Meghani, Vishal Kuvar, and Shiburaj Pappu.
%\newblock Wearable navigation and assistive system for visually impaired.
%\newblock In {\em 2018 2nd International Conference on Trends in Electronics
%  and Informatics (ICOEI)}, pages 747--751. IEEE, 2018.
%  
%\bibitem{brock2013supporting}
%Michael Brock and Per~Ola Kristensson.
%\newblock Supporting blind navigation using depth sensing and sonification.
%\newblock In {\em Proceedings of the 2013 ACM conference on Pervasive and
%  ubiquitous computing adjunct publication}, pages 255--258. ACM, 2013.
%
%\bibitem{huang2015indoor}
%Hsieh-Chang Huang, Ching-Tang Hsieh, and Cheng-Hsiang Yeh.
%\newblock An indoor obstacle detection system using depth information and
%  region growth.
%\newblock {\em Sensors}, 15(10):27116--27141, 2015.
%
%\bibitem{kulyukin2006robot}
%Vladimir Kulyukin, Chaitanya Gharpure, John Nicholson, and Grayson Osborne.
%\newblock Robot-assisted wayfinding for the visually impaired in structured
%  indoor environments.
%\newblock {\em Autonomous Robots}, 21(1):29--41, 2006.
%  
% \bibitem{huang2019augmented}
%Jonathan Huang, Max Kinateder, Matt~J Dunn, Wojciech Jarosz, Xing-Dong Yang,
%  and Emily~A Cooper.
%\newblock An augmented reality sign-reading assistant for users with reduced
%  vision.
%\newblock {\em PloS one}, 14(1):e0210630, 2019.
%
%\bibitem{westfechtel20163d}
%Thomas Westfechtel, Kazunori Ohno, B{\"a}rbel Mertsching, Daniel Nickchen,
%  Shotaro Kojima, and Satoshi Tadokoro.
%\newblock 3d graph based stairway detection and localization for mobile robots.
%\newblock In {\em 2016 IEEE/RSJ International Conference on Intelligent Robots
%  and Systems (IROS)}, pages 473--479. IEEE, 2016.
%  
%\bibitem{perez2014detection}
%Alejandro P{\'e}rez-Yus, Gonzalo L{\'o}pez-Nicol{\'a}s, and Jose~J Guerrero.
%\newblock Detection and modelling of staircases using a wearable depth sensor.
%\newblock In {\em European Conference on Computer Vision}, pages 449--463.
%  Springer, 2014.
%  
% \bibitem{westfechtel2018robust}
%Thomas Westfechtel, Kazunori Ohno, B{\"a}rbel Mertsching, Ryunosuke Hamada,
%  Daniel Nickchen, Shotaro Kojima, and Satoshi Tadokoro.
%\newblock Robust stairway-detection and localization method for mobile robots
%  using a graph-based model and competing initializations.
%\newblock {\em The International Journal of Robotics Research},
%  37(12):1463--1483, 2018.
%  
% \bibitem{souto2017stairs}
%Leonardo Souto, Andr{\'e} Castro, Luiz Gon{\c{c}}alves, and Tiago Nascimento.
%\newblock Stairs and doors recognition as natural landmarks based on clouds of
%  3d edge-points from rgb-d sensors for mobile robot localization.
%\newblock {\em Sensors}, 17(8):1824, 2017.
%  
% \bibitem{kuramochi2014recognition}
%Yusuke Kuramochi, Hotaka Takizawa, Mayumi Aoyagi, Nobuo Ezaki, and Mizuno
%  Shinji.
%\newblock Recognition of elevators with the kinect cane system for the visually
%  impaired.
%\newblock In {\em 2014 IEEE/SICE International Symposium on System
%  Integration}, pages 128--131. IEEE, 2014.
%  
%\bibitem{murata2018smartphone}
%Masayuki Murata, Dragan Ahmetovic, Daisuke Sato, Hironobu Takagi, Kris~M
%  Kitani, and Chieko Asakawa.
%\newblock Smartphone-based indoor localization for blind navigation across
%  building complexes.
%\newblock In {\em 2018 IEEE International Conference on Pervasive Computing and
%  Communications (PerCom)}, pages 1--10. IEEE, 2018.
%  
% \bibitem{bai2018virtual}
%Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, and Dijun Liu.
%\newblock Virtual-blind-road following-based wearable navigation device for
%  blind people.
%\newblock {\em IEEE Transactions on Consumer Electronics}, 64(1):136--143,
%  2018.
%  
%\bibitem{kacorri2018environmental}
%Hernisa Kacorri, Eshed Ohn-Bar, Kris~M Kitani, and Chieko Asakawa.
%\newblock Environmental factors in indoor navigation based on real-world
%  trajectories of blind users.
%\newblock In {\em Proceedings of the 2018 CHI Conference on Human Factors in
%  Computing Systems}, page~56. ACM, 2018.
%  
%\bibitem{liu2018augmented}
%Yang Liu, Noelle~RB Stiles, and Markus Meister.
%\newblock Augmented reality powers a cognitive assistant for the blind.
%\newblock {\em eLife}, 7:e37841, 2018.
%
%   
%\bibitem{kayukawa2019bbeep}
%Seita Kayukawa, Keita Higuchi, Jo{\~a}o Guerreiro, Shigeo Morishima, Yoichi
%  Sato, Kris Kitani, and Chieko Asakawa.
%\newblock Bbeep: A sonic collision avoidance system for blind travellers and
%  nearby pedestrians.
%\newblock In {\em Proceedings of the 2019 CHI Conference on Human Factors in
%  Computing Systems}, page~52. ACM, 2019.
%  
%\bibitem{pradeep2010robot}
%Vivek Pradeep, Gerard Medioni, and James Weiland.
%\newblock Robot vision for the visually impaired.
%\newblock In {\em 2010 IEEE Computer Society Conference on Computer Vision and
%  Pattern Recognition-Workshops}, pages 15--22. IEEE, 2010.
%  
% \bibitem{vision_based_mobile_indoor_nav_BingLi_2019}
%B.~{Li}, J.~P. {Muñoz}, X.~{Rong}, Q.~{Chen}, J.~{Xiao}, Y.~{Tian},
%  A.~{Arditi}, and M.~{Yousuf}.
%\newblock Vision-based mobile indoor assistive navigation aid for blind people.
%\newblock {\em IEEE Transactions on Mobile Computing}, 18(3):702--714, March
%  2019.
% 
%\bibitem{yang2018long}
%Kailun Yang, Kaiwei Wang, Shufei Lin, Jian Bai, Luis~M Bergasa, and Roberto
%  Arroyo.
%\newblock Long-range traversability awareness and low-lying obstacle
%  negotiation with realsense for the visually impaired.
%\newblock In {\em Proceedings of the 2018 International Conference on
%  Information Science and System}, pages 137--141, 2018.
%
%\bibitem{imai2017detecting}
%Kenta Imai, Itaru Kitahara, and Yoshinari Kameda.
%\newblock Detecting walkable plane areas by using rgb-d camera and
%  accelerometer for visually impaired people.
%\newblock In {\em 2017 3DTV Conference: The True Vision-Capture, Transmission
%  and Display of 3D Video (3DTV-CON)}, pages 1--4. IEEE, 2017.
%  
% \bibitem{bai2019wearable}
%Jinqiang Bai, Zhaoxiang Liu, Yimin Lin, Ye~Li, Shiguo Lian, and Dijun Liu.
%\newblock Wearable travel aid for environment perception and navigation of
%  visually impaired people.
%\newblock {\em Electronics}, 8(6):697, 2019.
%
% \bibitem{yang2016expanding}
%Kailun Yang, Kaiwei Wang, Weijian Hu, and Jian Bai.
%\newblock Expanding the detection of traversable area with realsense for the
%  visually impaired.
%\newblock {\em Sensors}, 16(11):1954, 2016.
% 
% \bibitem{aladren2014navigation}
%Aitor Aladren, Gonzalo L{\'o}pez-Nicol{\'a}s, Luis Puig, and Josechu~J
%  Guerrero.
%\newblock Navigation assistance for the visually impaired using rgb-d sensor
%  with range expansion.
%\newblock {\em IEEE Systems Journal}, 10(3):922--932, 2014.
% 
% \bibitem{lee2014wearable}
%Young~Hoon Lee and Gerard Medioni.
%\newblock Wearable rgbd indoor navigation system for the blind.
%\newblock In {\em European Conference on Computer Vision}, pages 493--508.
%  Springer, 2014.
% 
%\bibitem{wang2017enabling}
%Hsueh-Cheng Wang, Robert~K Katzschmann, Santani Teng, Brandon Araki, Laura
%  Giarr{\'e}, and Daniela Rus.
%\newblock Enabling independent navigation for visually impaired people through
%  a wearable vision-based feedback system.
%\newblock In {\em 2017 IEEE international conference on robotics and automation
%  (ICRA)}, pages 6533--6540. IEEE, 2017.
%  
%\bibitem{tapu2017deep}
%Ruxandra Tapu, Bogdan Mocanu, and Titus Zaharia.
%\newblock Deep-see: Joint object detection, tracking and recognition with
%  application to visually impaired navigational assistance.
%\newblock {\em Sensors}, 17(11):2473, 2017.  
%  
%\bibitem{yang2018automatic}
%Yang Yang, Shi Jin, Ruiyang Liu, Sing Bing~Kang, and Jingyi Yu.
%\newblock Automatic 3d indoor scene modeling from single panorama.
%\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
%  Pattern Recognition}, pages 3926--3934, 2018.
%  
% \bibitem{hulik2014continuous}
%Rostislav Hulik, Michal Spanel, Pavel Smrz, and Zdenek Materna.
%\newblock Continuous plane detection in point-cloud data based on 3d hough
%  transform.
%\newblock {\em Journal of visual communication and image representation},
%  25(1):86--97, 2014.
%  
%  \bibitem{zeineldin2016fast}
%Ramy~Ashraf Zeineldin and Nawal~Ahmed El-Fishawy.
%\newblock Fast and accurate ground plane detection for the visually impaired
%  from 3d organized point clouds.
%\newblock In {\em 2016 SAI Computing Conference (SAI)}, pages 373--379. IEEE,
%  2016.
%  
%
%\bibitem{strom2010graph}
%Johannes Strom, Andrew Richardson, and Edwin Olson.
%\newblock Graph-based segmentation for colored 3d laser point clouds.
%\newblock In {\em 2010 IEEE/RSJ International Conference on Intelligent Robots
%  and Systems}, pages 2131--2136. IEEE, 2010.
%  
% \bibitem{holz2011real}
%Dirk Holz, Stefan Holzer, Radu~Bogdan Rusu, and Sven Behnke.
%\newblock Real-time plane segmentation using rgb-d cameras.
%\newblock In {\em Robot Soccer World Cup}, pages 306--317. Springer, 2011.
%
%\bibitem{redmon2016you}
%Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
%\newblock You only look once: Unified, real-time object detection.
%\newblock In {\em Proceedings of the IEEE conference on computer vision and
%  pattern recognition}, pages 779--788, 2016.
%
%\bibitem{howard2017mobilenets}
%Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
%  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
%\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
%  applications.
%\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.
%  
% \bibitem{DBLP:journals/corr/abs-1811-05588}
%Jonathan Pedoeem and Rachel Huang.
%\newblock {YOLO-LITE:} {A} real-time object detection algorithm optimized for
%  non-gpu computers.
%\newblock {\em CoRR}, abs/1811.05588, 2018.
%
%\bibitem{zhang2018shufflenet}
%Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
%\newblock Shufflenet: An extremely efficient convolutional neural network for
%  mobile devices.
%\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
%  Pattern Recognition}, pages 6848--6856, 2018.
%  
% \bibitem{grunnet2018depth}
%Anders Grunnet-Jepsen and Dave Tong.
%\newblock Depth post-processing for intel{\textregistered} realsense™ d400
%  depth cameras.
%  
%\bibitem{huang2017speed}
%Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,
%  Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama,
%  et~al.
%\newblock Speed/accuracy trade-offs for modern convolutional object detectors.
%\newblock In {\em Proceedings of the IEEE conference on computer vision and
%  pattern recognition}, pages 7310--7311, 2017.
%  
%
%\bibitem{sandler2018mobilenetv2}
%Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
%  Chen.
%\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
%\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
%  Pattern Recognition}, pages 4510--4520, 2018.  
%   
%\bibitem{ren2015faster}
%Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
%\newblock Faster r-cnn: Towards real-time object detection with region proposal
%  networks.
%\newblock In {\em Advances in neural information processing systems}, pages
%  91--99, 2015.  
%  
%\bibitem{liu2016ssd}
%Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
%  Cheng-Yang Fu, and Alexander~C Berg.
%\newblock Ssd: Single shot multibox detector.
%\newblock In {\em European conference on computer vision}, pages 21--37.
%  Springer, 2016.
%  
% \bibitem{edworthy1991improving}
%Judy Edworthy, Sarah Loxley, and Ian Dennis.
%\newblock Improving auditory warning design: Relationship between warning sound
%  parameters and perceived urgency.
%\newblock {\em Human factors}, 33(2):205--231, 1991. 
%
%  
%\end{thebibliography}


\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/zhongen.png}}]{Zhongen Li} received the B.S. degree in information and computing science from Anhui Jianzhu University, Hefei, China, in 2013 and the M.S. degree in integrated circuit engineering from University of Science and Technology of China, Hefei, China, in 2016. He is currently pursuing the Ph.D. degree in computer engineering at Ohio University, Athens, OH, USA. His current research interests include object detection in 2D and 3D, and natural language processing.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/fanghao.png}}]{Fanghao Song} 
received the B.S. degree in heat energy dynamics engineering (thermal engineering) from Shenyang Aerospace University, Shenyang, China, in 2014 and the M.S. degree in mechanical engineering from the University of Southern California, CA, USA, in 2016. He is currently pursuing the Ph.D. degree in computer science at Ohio University, Athens, OH, USA. His current research interests focus on health applications and indoor navigation software development. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Clark.png}}]{BRIAN C. CLARK} received the B.S. degree in biology from Western Carolina University in Cullowhee, NC, USA in 1998, a M.S. degree in exercise physiology (2001) and a Ph.D. in neuromuscular physiology (2005) from Syracuse University in Syracuse, NY, USA.

He is the Clybourne Endowed Research Chair and Professor of Physiology and Neuroscience in the Department of Biomedical Sciences at Ohio University, Athens, OH, USA where he also is a Principal Investigator and Executive Director of the Ohio Musculoskeletal and Neurological Institute. The overall goal of his research is to develop effective and implementable interventions that increase muscle function and mobility in patients with orthopedic and neurologic disabilities. He has published more than 135 peer reviewed articles in clinical, basic science, and engineering journals.

Dr. Clark’s awards and honors include receiving the All-University Doctoral Prize from Syracuse University (2006), serving on dozens of national and international grant review panels including being a stand member of the NIH’s Motor Function and Speech Rehabilitation Study Section (2014-2020), and serving on numerous expert panels and round tables for federally- and industry sponsored projects.
 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Grooms.png}}]{Dustin R. Grooms} received bachelor’s degree in Athletic Training from Northern Kentucky University, Highland Heights, KY, USA, in 2008 and the M.S. degree in kinesiology from the University of Virginia, Charlottesville, VA, USA, in 2009, and the Ph.D. degree in health and rehabilitation sciences from the Ohio State University, Columbus, OH, USA, in 2015. 

Prior to pursuing doctoral studies, he was a rehabilitation clinician and instruct at the College of Mount Saint Joseph in Cincinnati Ohio. He is an Associate Professor in the Division of Athletic Training in the School of Applied Health Sciences and Wellness at Ohio University. His current main research interest is how the brain and movement mechanics change after injury and the design of novel therapies.


\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Liu_Chang.png}}]{Chang Liu} received the Ph.D. degree in Information and Computer Science from the University of California, Irvine, CA, USA, in 2002. 

He is an IEEE senior member and currently a professor in computer science at Ohio University. His research focuses on software engineering and its applications in various domains, including learning and medical applications. 
\end{IEEEbiography}

\EOD

\end{document}
