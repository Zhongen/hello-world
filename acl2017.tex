\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{comment}
\usepackage{float}
\graphicspath{ {./images/} }
\usepackage{subcaption}
% \usepackage{textcomp}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage{tabularx}
\usepackage{multirow}
% \SetKwComment{Comment}{$\triangleright$\ }{}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}




\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}


\title{A Wearable Device for Indoor Imminent Danger Detection and Avoidance with Region-based Ground Segmentation}
\author{\uppercase{Zhongen Li}\authorrefmark{1}, 
\uppercase{Fanghao Song\authorrefmark{1}, 
Brian Clark\authorrefmark{2}  \authorrefmark{3}, 
Dustin Grooms\authorrefmark{4}, and Chang Liu}.\authorrefmark{1},
\IEEEmembership{Senior Member, IEEE}}
\address[1]{School of Electrical and Computer Engineering, Ohio University, Athens, OH 45701 USA}
\address[2]{Ohio Musculoskeletal and Neurological Institute, Ohio University, Athens, OH 45701 USA}
\address[3]{Department of Biomedical Sciences, Ohio University, Athens, OH 45701 USA}
\address[4]{College of Health Sciences and Professions, Ohio University, Athens, OH 45701 USA}
\markboth
{Zhongen Li \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Zhongen Li \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
\corresp{Corresponding author: Chang Liu (e-mail: liuc@ohio.edu).}

\begin{abstract}
Avoiding objects independently in indoor environments for individuals with severe visual impairment is one of the significant challenges in daily life. This paper presents a wearable application to help visually impaired people quickly build situational awareness and traverse safely. The system utilizes Red, Green, Blue, and Depth (RGB-D) camera and an Inertial Measurement Unit (IMU) to detect objects and the collision-free path in real-time. A region proposal module is presented to decide where to identify the ground from 3D point clouds. The segmented ground area can act as the traversable path, and its corresponding region in the image is removed to prevent detecting painted objects. The system can provide information about the category, distance, and direction of the detected objects by fusing the depth image and the neural network results. A 3D acoustic feedback mechanism is designed to improve the situational awareness for visually impaired people, and guild them traverse safely. The advantage of this system is that our 3D region proposal module can robustly propose the potential ground region and greatly reduce the computation cost of the ground segmentation. Besides, a typical machine-learning-based approach may miss objects because they could not be recognized, though they still may pose a danger. Another advantage of our approach is that the imminent danger detector can detect such unrecognizable objects to help users avoid a collision. Finally, experiments by blindfolded subjects demonstrate that the proposed system provides a useful indoor assistant tool to help blind individuals with collision avoidance and wayfinding.





\end{abstract}

\begin{keywords}
Convolutional neural network, ground segmentation, object detection, point cloud, region of interest, visual impairment, wearable assistive system.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
\PARstart{A}ccording to the World Health Organization (WHO) report\cite{WHO}, there are no less than 2.2 billion people with visual impairment or blindness worldwide. Severely visually impaired individuals (i.e., those with best-corrected visual acuity worse than 20/200 in the best-seeing eye or those with less than 20 degrees of the functional visual field) face many challenges and risks in navigating their environments. These challenges include performing autonomous navigation and environmental perception in unfamiliar indoor environments. When entering complex buildings such as large shopping malls, university buildings, or hospitals, one must perceive the surrounding environment and avoid objects to reach the destination safely. For the blind and visually impaired, accurate object detection and timely feedback are required to help them complete indoor activities. Therefore, these challenges add to the classic problems of spatial navigation and obstacle avoidance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{images/prototype1.png}
    \caption{The proposed system prototype.}
    \label{fig:prototype}%
\end{figure}

Traditionally, visually impaired people usually rely on navigational aids, namely guild dogs \cite{BhatlawandeDesign} and canes \cite{7879309} for help when detecting obstacles. However, they may still suffer injuries from hanging obstacles due to the limited detecting range of white canes and guide dogs. To improve mobility and safety for visually impaired people, advanced sensing techniques have adopted (e.g., ultrasonic sensors \cite{shoval1998navbelt} \cite{shaikh2018wearable}, visual sensors \cite{brock2013supporting} \cite{huang2015indoor}, and laser emitters \cite{kulyukin2006robot}). However, ultrasonic sensors are poor at target identification or localization. Besides, laser emitters are expensive, heavy, and require high power consumption, which makes them unsuitable for wearable and affordable applications. Vision-based assistants, such as monocular cameras, stereoscopic cameras, and Red, Green, Blue, and Depth (RGB-D) cameras, have been widely used for target finding, localization, navigation, etc. For example, an important function of vision-based applications is to help visually impaired users find specific objects to use, such as chairs \cite{takizawa2019kinect}, sign patterns in road environments, markers pasted on podiums and classroom doors \cite{huang2019augmented}, up/down stairways \cite{westfechtel20163d} \cite{perez2014detection} \cite{westfechtel2018robust} \cite{souto2017stairs}, and elevators \cite{kuramochi2014recognition}. For other applications, like visual simultaneous localization and mapping (VSLAM) for blind navigation has become a popular research field \cite{murata2018smartphone} \cite{bai2018virtual} \cite{kacorri2018environmental} \cite{liu2018augmented} as well. Because these applications assume object avoidance technology has been adopted, the visually impaired can reach their destinations independently and safely. Therefore, a wearable real-time object avoidance system is needed to assist the visually impaired in navigation and environmental perception.


With the rapid and widespread development of sensor technology, RGB-D cameras become affordable and portable. They can directly provide three-dimensional environmental information without performing the computational depth calculations required by RGB cameras.
For the RGB-D based object detection system, numerous studies have been conducted in the past years \cite{kayukawa2019bbeep} \cite{vision_based_mobile_indoor_nav_BingLi_2019} \cite{aladren2014navigation} \cite{lee2014wearable}. However, indoor assistive object detection still faces many challenges: low-cost and effective obstacle recognition and avoidance, real-time and rich feedback, and the complexity of a holistic system on compact portable devices for visually impaired users.
This paper proposes a wearable device for indoor object detection and avoidance system with region-based ground segmentation to improve mobility for visually impaired people. The proposed system utilizes an RGB-D camera as the perceptual sensors to extract information about the surroundings, such as the available free path, and the types and directions of identified obstacles. An inertial measurement unit (IMU) is combined to help track the real-time orientation of the camera in use. Through complementary fusion, more accurate and richer environmental information can be obtained from point clouds and images.



All calculations are completed locally in real-time. Hence no internet connection is needed, which might have privacy and access issues. The solution provides 3D sound feedback through speakers to offer accurate situational awareness and imminent danger alerts for the visually impaired. With a miniaturized design, as shown in Fig. \ref{fig:prototype}, the system can reduce the risk of catastrophic injury, thereby improving indoor mobility and quality of life.

This main contributions of this paper are:
\begin{itemize}
    \item A wearable indoor object detection and avoidance system can enhance the safe mobility of visually impaired people;
    
    \item A region-based ground segmentation can robustly segment the ground and reduce calculation time;
    
    \item Object detection fusion from both point clouds and images to ensure accurate detection results;
    
    \item A 3D acoustic feedback mechanism optimized for different priorities to improve the situational awareness for blind people.

\end{itemize}


The rest of the paper is structured as follows. Section \ref{section:relatedwork} reviews various RGB-D based object recognition systems, ground segmentation approaches, and obstacle avoidance methods. Section \ref{section:framework} elaborates on the principle components of the proposed framework. After that, Section \ref{section:experiment} shows the experimental evaluations and discussions. Finally, the conclusions, along with future work, are discussed in the end.

\section{Related Work} \label{section:relatedwork}
The usage of robotic and autonomous driving techniques has been widely approached to aid safe navigation for visually impaired people. This research builds on important related work to the object recognition systems, ground segmentation approaches, and local obstacle detection solutions.

\subsection{RGB-D based object recognition systems}
Since RGB-D based devices have advantages over other sensing devices as discussed above, in this section, we focus on the object recognition system with RGB-D sensors.   

Kayukawa et al. \cite{kayukawa2019bbeep} developed a vision-sensing system, BBeep, for clearing the path in front of a blind user. This work was able to detect pedestrians and predict their future positions based on consecutive frames. By using 3D odometry API,  the system could remove the influence of the suitcase rotation in real-time. However, this system is not suitable to carry out for blind users because it was implemented based on a suitcase. 

Pradeep et al. \cite{pradeep2010robot} proposed a wearable navigation system with obstacle detection and avoidance functionalities. Obstacles were detected by scanning 2D grids neighborhood quantized from the 3D point cloud. The system used haptic feedback to alert users when obstacles were detected in front. However, the system lacks obstacle modeling to recognize the category of objects. While object detection is only considered during the path planning phase, the system does not support real-time obstacle avoidance.

Li et al. \cite{vision_based_mobile_indoor_nav_BingLi_2019} proposed an intelligent situational awareness and navigation aid, which was based on a Google Project Tangle device to help visually impaired people traverse indoor area independently. Dynamic planning was performed by real-time perception with an RGB-D camera. Based on a time-stamped occupancy map, real-time object detection was implemented through two 2D projections. This method has a lower cost but less information. For example, the type and shape of objects can not be determined. The detection of long-range ground and obstacles for visually impaired people was presented in \cite{yang2018long}. Although the proposed system improves the pathfinding task for blind and visually impaired, it also does not provide any category information of the detected objects in the user’s surroundings.

 \begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{images/System_workflow.png}
    \caption{The workflow of the proposed system.}
    \label{fig:System_workflow}
\end{figure}

\begin{table*}
\caption{Comparison of different object detection and avoidance systems. $\checkmark$ := supported,$\times $ := unsupported, $\sim$:= unclear from the publication.}
\label{tab:publication comparison}
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\hline
 & Sensoring Devices  & Wearable &\begin{tabular}[c]{@{}c@{}}Real\\ Time \end{tabular}&\begin{tabular}[c]{@{}c@{}}Orientation \\ Tracking\end{tabular} &\begin{tabular}[c]{@{}c@{}}Height\\ Estimation \end{tabular}&\begin{tabular}[c]{@{}c@{}}Ground\\ Segmentation \end{tabular} & \begin{tabular}[c]{@{}c@{}}Object \\ Recognition\end{tabular}  & Feedback\\ \hline
Kayukawa et al. \cite{kayukawa2019bbeep} &	RGB-D	&	$\times $	&	$\checkmark$		&$\times $&$\times $&$\times $&$\checkmark$& Acoustic	\\
Pradeep et al. \cite{pradeep2010robot}  &  Stereo Camera $\&$ IMU &$\checkmark$&	$\checkmark$&$\checkmark$&$\checkmark$&$\times $& $\times $ & Acoustic\\
Li et al. \cite{vision_based_mobile_indoor_nav_BingLi_2019}&RGB-D $\&$ IMU&$\checkmark$&	$\checkmark$ &$\checkmark$&$\checkmark$   &$\checkmark$ &$\times $& Acoustic\\
Yang et al. \cite{yang2018long} &RGB-D $\&$ IMU &$\checkmark$&	$\checkmark$&$\checkmark$ &$\times $& $\checkmark$&$\times $&\begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular}  \\
Imai et al. \cite{imai2017detecting} & RGB-D $\&$ Accelerometer&$\sim$&$\sim$&	$\checkmark$&$\checkmark$&$\checkmark$&  $\times $& $\times $\\
Takizawa et al. \cite{takizawa2019kinect} &RGB-D &$\times$&$\sim$&$\times$& $\times$& $\times$ &$\checkmark$ &\begin{tabular}[c]{@{}c@{}}Acoustic\\ $\&$Haptic\end{tabular} \\ 
Bai et al. \cite{bai2019wearable}&RGB-D $\&$ IMU&$\checkmark$ &$\checkmark$& \begin{tabular}[c]{@{}c@{}}Pitch-angle\\ Only \end{tabular} &$\times$ &$\checkmark$&$\checkmark$&Acoustic\\ 
Yang et al. \cite{yang2016expanding}&RGB-D$\&$ IMU&$\checkmark$&$\sim$ &$\checkmark$&$\times$&$\checkmark$ &$\times$& \begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular} \\ 
Aladren et al. \cite{aladren2014navigation} & RGB-D & $\checkmark$& $\sim$ & $\times$&$\times$ &$\checkmark$& $\times$&Acoustic\\ 
Lee et al. \cite{lee2014wearable}&RGB-D $\&$ IMU&$\checkmark$& $\times$ &  $\checkmark$ &  $\checkmark$& $\checkmark$&  $\times$ &   Tactile              \\ 
Wang et al. \cite{wang2017enabling}  &   RGB-D  & $\checkmark$&$\checkmark$&$\times$ &$\times$ &$\checkmark$& $\checkmark$&  \begin{tabular}[c]{@{}c@{}c@{}}Braille \\ $\&$Haptic \\ $\&$Acoustic\end{tabular} \\ 
Tapu et al. \cite{tapu2017deep}   & Smart-phone  &$\checkmark$&$\checkmark$& $\times$ &$\times$ &$\times$&$\checkmark$& \begin{tabular}[c]{@{}c@{}}Bone-\\ Conduction\end{tabular}\\
Ours                  &RGB-D $\&$ IMU & $\checkmark$ &  $\checkmark$& $\checkmark$&$\checkmark$ &$\checkmark$&$\checkmark$& Acoustic\\ \hline
\end{tabular}
\end{table*}


\subsection{Ground Segmentation from Point Clouds}

Ground segmentation is the first step in object detection because it aims to distinguish between viable ground areas and obstacles. Besides, visually impaired people can rely on the detected ground as the traversable road areas and search the optimal route \cite{yang2018automatic}. Imai et al. \cite{imai2017detecting} proposed an approach to identify the walkable path by using an RGB-D camera and an accelerometer. Their work estimated the initial orientation by decomposing the gravity on the tri-axis. Besides, the ground height and normal vectors were taken into consideration when detecting ground. However, the ground height calculation by one depth column is error-prone, if an obstacle happened to exist in there.

The Random Sample Consensus (RANSAC) is an iterative method and can be used for robustly estimating plane parameters from a set of data points. Takizawa et al. \cite{takizawa2019kinect} used RANSAC algorithm to extract wall and ground for helping blind people find an available seat. Bai et al. \cite{bai2019wearable} applied RANSAC for coarse ground detection. Then a time-dependent adaptive ground height segmentation algorithm among adjacent frames was adopted to refine the ground result. In \cite{yang2016expanding}, the traversable area preliminarily was obtained using RANSAC segmentation. Then, the preliminary traversable area was further enlarged by a growing region algorithm. However, these works treat the camera as static, and real-time orientation tracking is not considered. 

3D Hough transform (HT) for plane detection from point clouds were proposed in \cite{hulik2014continuous}. HT describes every plane by its slope. The points in the parameter space will correspond to planes detected in the object space. After processing is finished, the accumulator in the parameter space will search the satisfying point, which corresponds to the shape of interest plane in the original space. Although HT is less accurate than the RANSAC, it needs less time to detect planes \cite{zeineldin2016fast}.
 
 
There are other methods, like using graph descriptor, surface normal, or color information, to detect ground from the point cloud. Graph-based ground segmentation was proposed using a spatial mesh \cite{strom2010graph}. Aladren et al. \cite{aladren2014navigation} used both depth and color images to detect the ground. Although their system achieves high ground detection accuracy, it is too computationally expensive to use in real-time.  Holz et al. \cite{holz2011real} calculated the local surface normal on the integral image. Then, these points were clustered and segmented in the normal space.




\subsection{Obstacle Avoidance Approaches}
Lee et al. \cite{lee2014wearable} created a 2D probabilistic occupancy map to avoid obstacles in real-time during assistive navigation. The system built a 3D voxel map of the environment to analyze the traversability. Then, a 2D traversable grid map was used to support dynamic path planning. Point cloud clustering for Object detection were presented in \cite{wang2017enabling} \cite{vision_based_mobile_indoor_nav_BingLi_2019}. First, the detected ground was removed, and then the Euclidean segmentation was performed on the remaining points to cluster the objects. If the distance between two points was less than the threshold, they were merged. Finally, the algorithm's output was a set of object clusters, where each cluster was a collection of points of the same object. However, these methods are highly dependent on the size threshold of the merged objects and cannot provide category information to which the objects belong.



Object recognition based on machine learning methods has become increasingly popular. Tapu et al. \cite{tapu2017deep} proposed a DEEP-SEE system to detect both dynamic and static objects using the You-Only-Look-Once (YOLO) object recognition method \cite{redmon2016you}. The system can provide the obstacle category and distance information. However, this neural network needs heavy computation overhead, making it hard to fulfill real-time detection on mobile devices. Lightweight image-based neural networks have proposed to increase the accessibility of real-time object detection to mobile devices, such as MobileNet \cite{howard2017mobilenets}, YOLO-LITE \cite{DBLP:journals/corr/abs-1811-05588}, ShuffleNet \cite{zhang2018shufflenet} etc. However, these lightweight algorithms do not consider distance information directly when performing object recognition. So, the painted object on the ground might be detected to mislead the visually impaired people.

To overcome the shortcoming of lightweight image-based algorithms, Bai et al. \cite{bai2019wearable} proposed a 2.5-D object detection, which combined MobileNet object detection and depth image based object detection. The intersection between their mapped areas was used to improve the detection result. However, refinement by intersection may miss objects because they could not be recognized by a machine-learning-based approach even though they still may pose a danger. 

Table. \ref{tab:publication comparison} summarizes the functions and limitations of different assistive systems for visually impaired people. Compared with these works, our system is versatile. Not only can it dynamically track the orientation and altitude of the camera to segment the ground surface, but also provide real-time avoidance of obstacles and provide category information using a portable device.

\section{The Proposed Framework} \label{section:framework}

As shown in Fig. \ref{fig:System_workflow}, the proposed system first obtains RGB and depth images from an RGB-D camera and gets acceleration and angular velocity from the IMU. Then, the real-time orientation and height of the camera are estimated by the enhanced depth image and IMU data. The system lifts RGB images and depth maps to 3D point clouds and reconstructs the actual 3D scene based on the orientation and height tracker. Then the potential ground area's region of interest (ROI) is proposed, thus the ground plane can be segmented effectively to search for available paths and objects are detected over the ground. To further increase the stability, a feedback calibration for orientation and height estimation is presented.  The final objects with type, distance, and direction are calculated by fusing the output of the real-time CNN object detector and the corresponding depth image. To enhance system reliability, an imminent danger detector is implemented to capture the unrecognizable objects. The region corresponding to the ground area from the point cloud is removed from images before feeding into the CNN object detector, which reduces false-positive results such as painted objects on the ground. The system provides informative 3D acoustic feedback to users according to potential hazards and the feasible path that it has identified. The main algorithms of the proposed framework are described in detail in the following subsections.




\subsection{Depth Pre-processing}
The raw depth data output by the RGB-D camera may have "holes" and noise, which negatively affects system performance. Therefore, the pre-processing on depth is necessary to get better performance of the point cloud. This module comprises a decimation filter, a spatial filter, and a temporal filter to reduce data amount, fill holes, and improve noise while preserving features. 


Since the amount of data plays a crucial role in application performance, downsampling could reduce the density of points in very dense areas, but has little effect on sparse areas. Non-zero downsampling is performed by taking the average of neighbor pixels while ignoring zeros. The bilateral filter is used to smooth depth noise while preserving edges by calculating the one-dimensional exponential moving average (EMA). The recursive equation of EMA is represented as: 
\begin{equation}
 S_t=\left\{
\begin{array}{lc}
Y_1                                     &       t=1\\
\alpha Y_t +(1-\alpha)S_{t-1}           & t>1 \  and \  S_{t-1}- S_{t}\ <\ \delta \\
Y_t                                     & t>1 \ and\  S_{t-1}- S_{t}\ >\ \delta

\end{array} \right. 
\end{equation}
Where the coefficient $\alpha$ represents the degree of the weighting factor. $Y_t$ is the instantaneous value, $\delta$ is the threshold, and $S_t$ is the value of the EMA at time t.

% \begin{figure}[b]
%     \centering
%     \subfloat[]{{\includegraphics[width=0.4\linewidth]{images/rgb_Color.png} }}
%     \qquad
%     \subfloat[]{{\includegraphics[width=0.4\linewidth]{images/original_Depth.png} }}
%     \caption{An example of the raw data of the camera. (a) Color image. (b) Depth image with noises and black "holes".}
%     \label{fig:hole_in_depth_image}
% \end{figure}
\begin{table}[b]
\centering
\caption{Parameters of the depth pre-processing module.}
\label{tab:depth-preprocessing-parameters}
\begin{tabular}{|c|c|}
\hline
Filters    & Parameters                                                                                                               \\ \hline
Decimation & sub-sample = 5                                                                                                           \\ \hline
Spatial    & \begin{tabular}[c]{@{}c@{}}EMA: $\alpha = 0.25, \delta=20$\\ number of adjacent pixels for hole filling = 2\end{tabular} \\ \hline
Temporal   & number of frames for hole filling = 2                                                                                    \\ \hline
\end{tabular}
\end{table}

In a stereoscopic system, holes exist when either data being unavailable or not having met a confidence metric (e.g., occlusion, lack of texture .etc). In this situation, the camera sets the depth of these holes to zero instead of providing the wrong value. As shown in Fig. \ref{fig:depth-pre-processing} (b), the holes are displayed as black dots. The method of filling holes is a combination of spatial and temporal filters. The holes are filled first with valid adjacent pixels within the specified radius in the same frame. Besides, if holes still occur, they are filled with the last valid (if they exist) value by looking at several frames back in history.  
\begin{figure}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/rgb_Color.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/original_Depth.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/pre_process_Depth.png} }}%
    \caption{Results of depth image enhancement. (a) Raw color image. (b) Raw depth image with noises and black "holes". (c) The enhanced depth image after pre-processing.}%
    \label{fig:depth-pre-processing}
\end{figure}

Based on the recommendation from \cite{grunnet2018depth}, the RealSense D435i camera is streaming depth data at 840 by 480 resolution and with the infrared sensor on to have the best quality of the depth image. In the depth pre-processing module, the parameters of filters are listed in Table. \ref{tab:depth-preprocessing-parameters}. The raw depth image is shown in Fig. \ref{fig:depth-pre-processing} (b), which has noises and holes. After processing by decimation filter, spatial filter, and temporal filter, the quality of the depth images is improved with few or no holes, while preserving features, shown in Fig. \ref{fig:depth-pre-processing} (c).

\subsection{Ground region proposal}
Since the point cloud has the feature of 3D Euclidean measurements in front of the camera, it is essential to calculate the relative position between the camera and the object to understand the absolute position of objects. We provide real-time orientation and altitude trackers to help calculate the relative position. In our implementation, we only consider the pitch and roll angles. Then, the scene is reconstructed based on the relative position. Besides, the potential ground area is suggested based on the height of the plane in the reconstructed 3D scene.

 
\subsubsection{Orientation and height tracker} \label{section:orientation tracker}

 \begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]  {images/InitialOrientation.png}
    \caption{Definition of Euler angles. Left: the camera with zero degrees of Euler angles; Middle: the pitch angle, which is respect to Z-axis while clock-rise rotating around X-axis; Right: the roll angle is regarding X-axis while clock-rise rotating around Z-axis.}
    \label{fig:InitialOrientation}
\end{figure}

Orientation and height estimation within the world coordinate space play a principal role in proposing the region of interest. As shown in Fig. \ref{fig:InitialOrientation}, the camera is centered at a right-handed Cartesian coordinate system, where the positive X, Y, and Z-axis are defined as the cameras left direction, up direction, and facing direction,  respectively. We use $A_{X,OUT}$ as the acceleration along the X-axis, subsequently $A_{Y,OUT} $, and $A_{Z,OUT} $ are accelerations in Y and Z axes. The initial orientation of the camera is calculated by decomposing gravity g from three-axis accelerations, which is represented as:  

\begin{equation}
Pitch = tan^{-1}(\frac{\sqrt{A^2_{X,OUT}+A^2_{Y,OUT}}}{A_{Z,OUT}})
\label{equation: pitch}
\end{equation}

\begin{equation}
Roll = tan^{-1}(\frac{A_{X,OUT}}{\sqrt{A^2_{Y,OUT}+A^2_{Z,OUT}}})
\label{equation: roll}
\end{equation}




In theory, the real-time orientation can be gauged through integrating the output of the gyroscope. However, the estimation results suffer from the integration of drift over time. To help fix the time drift of the gyroscope, we use the complementary filter to allow short-duration signals from the gyroscope to pass through while blocking steady signals. The noise and horizontal acceleration dependency are also alleviated by the complementary filter, which is formulated as:
\begin{equation}
Angle(t) = \alpha  (Angle(t) + \int{gyro(t)}  dt) + (1-\alpha)  acc(t)
\label{equation: Complementary Filter}
\end{equation}
Where $\alpha$ is the filtering weight, $gyro(t)$ is the output of the gyroscope at time $t$, and $acc(t)$ is the output of the accelerometer at time $t$.

The only knowledge about the camera pose is the approximate position of the camera on the chest because the subject’s height and movement are continually changing. The initial camera height estimation is equivalent to calculating the distance from the camera position to the ground. The RANSAC program is executed in the initial system setup to search the ground. During the system setup, the camera is facing down statically, and an additional pass filter on the y-axis can be used to limit the search of the ground plane. Repeat the process until the floor is detected. Since the user should stand on the floor, this process will not last long.

Once the ground plane is determined, it can be represented as \eqref {equation: plane model}, where A, B, C, and D are the coefficients of the ground plane. From the point-plane distance, the height of the camera $H_{camera}$ can be computed as \eqref{equ:height}. Besides, due to the bias and noise presented in IMU, the normal vector $n$ (e.i., $(A, B, C)$) is used to calibrate the estimated initial pitch angle. Real-time altitude estimation is 
achieved using feedback calibration, which will be explained in detail later.
 \begin{equation}
Ax + By + Cz + D = 0 
\label{equation: plane model}
\end{equation}

\begin{equation}
H_{camera}=\frac{D}{\sqrt{A^2+B^2+C^2}}
\label{equ:height}
\end{equation}


\subsubsection{3D reconstruction} 
As long as the orientation and the height of the camera are estimated, the 3D scene can be reconstructed. The rotation angles of interest are pitch angle ($\theta$) and roll angle ($\alpha$) corresponding to the x-axis and the z-axis. The rigid transformation from the camera coordinate system to the world coordinate system is achieved by a combination of a rotation matrix $\textbf{R}$ and a transition vector $\textbf{t}$.  The rotation matrix is used to make the normal vector $n$ parallel to $(0, 1, 0)$, and the transition matrix is used to move a point to the origin. The affine transformation matrix $\textbf{T}$ in the homogeneous coordinate is represented as:



\begin{equation}
\begin{aligned}
\textbf{T} &=
\begin{bmatrix}
  \textbf{R} & \textbf{t}\\ 
  \textbf{0}^T & 1
\end{bmatrix}
= 
\begin{bmatrix}
  \textbf{R}_z\textbf{R}_x & \textbf{t}\\ 
  \textbf{0}^T & 1
\end{bmatrix} 
\\
&=
\begin{bmatrix}
  \cos{\theta} & -\sin{\theta}\cos{\alpha} & \sin{\theta}\sin{\alpha} & 0\\ 
  \sin{\theta} & \cos{\theta}\cos{\alpha} & -\cos{\theta}\sin{\alpha} & -H_{camera} \\
  0 & \sin{\alpha} &\cos{\alpha}  & 0 \\
  0 & 0 & 0 & 1
\end{bmatrix}
\label{Equ:transform}
\end{aligned}
\end{equation}

where: 
\begin{equation}
\begin{aligned}
R_z&=
\begin{bmatrix}
   \cos{\theta} & -\sin{\theta} & 0 \\ 
  \sin{\theta} & \cos{\theta}   & 0\\
  0 & 0 &1
\end{bmatrix} \\
R_x&=
\begin{bmatrix}
   1& 0 & 0 \\ 
  0 & \cos{\alpha}   & -\sin{\alpha}\\
  0 & \sin{\alpha} & \cos{\alpha}
\end{bmatrix} \\
 \textbf{t} &= 
\begin{bmatrix}
 0\\ 
 -H_{camera}  \\
 0 
\end{bmatrix}
\label{Equ:roatation}
\end{aligned}
\end{equation}
After the original point cloud is reconstructed based on the real-time orientation and height trackers, the absolute position of objects in the scene is obtained. Since the ground has the following characteristics: parallel to the XZ plane and the intersection point on the y-axis is 0, a band-pass filter is used to propose a ground ROI along the y-axis, for example, from -0.1 meters to 0.1 meters. The advantage of searching the ground in the ROI is that it dramatically reduces the 3D search space for the RANSAC algorithm and saves the calculation time.

\subsection{Ground Segmentation}
Ground segmentation is essential to understand the surrounding environment because the segmented ground can be severed as the three purposes: 

\begin{itemize}
\item Wayfinding for users and use the segmented ground area as the free movable area;
\item The parameters of the ground plane can be used to calibrate the orientation and height tracker;
\item Applying object detection over the segmented ground. 
\end{itemize} 

\subsubsection{Ground segmentation}
\begin{algorithm}
\SetAlgoLined
\caption{Ground segmentation in point cloud based on RANSAC algorithm.}
\label{alg:ransac}

\KwIn{\\
$c$: input point cloud \\
$n$: normals of the input point cloud  \\
$threshold$: distance threshold to the model  \\
$axis$ : surface normal for plane estimation \\
$\theta_{th}$: the maximum allowed difference between the model normal and the given axis \\
$M$: max iterations}     
\KwOut{\\
$inliers$: the point list of the segmented plane \\
$bestPlane$: the coefficients of the segmented plane } 
 
\textbf{procedure} RANSAC($c, n, threshold, axis, \theta_{th}, M$) \\
\Indp
\textbf{initialization:}  \\
$i \leftarrow 0, bestSupport \leftarrow 0$\; 
$ bestPlane  \leftarrow [0, 0, 0, 0], bestStd  \leftarrow \infty$\; 
\While {i <= M}{
	$j$ = sample 3 points randomly among $c$\;
	$plane$ = points2plane($j$)\;
	$diff$ = diff2plane($plane$, $axis$)\;
	\If{$diff$ < $\theta_{th}$}{
	    $dis$ = dis2plane($plane$, $c$) \\
	    $inliers$ = find($abs(dis)<=threshold$) \\
	    $st$ = standard\_deviation($inliers$) \\
    	\eIf {length($inliers$) > $bestsupport$ or (length($inliers$) = $bestsupport$ and $st$ < $bestStd$)}{
	       $bestsupport$ = length($inliers$)\;
	        $bestplane$ = $pl$\;
	        $bestStd$ = $st$\;
	    }
	    {$i$=$i$+1\;}
    }
}
\textbf{return} $bestPlane$
\end{algorithm}

To segment the ground, we adopt the guaranteed RANSAC algorithm by adding additional constraints to the ROI out of the region proposal module. 
Algorithm \ref{alg:ransac} specifies the pseudo-code of the RANSAC algorithm for ground segmentation. The segmentation starts from randomly choosing a set of 3D points to estimate the parameters A, B, C, and D in  \eqref{equation: plane model}. The function $point2plane$ calculates the distance from a point to the estimated plane. The remaining points are evaluated through \eqref{equ:inliers} to count the number of inliers (T is the distance threshold). After certain iterations, the ground plane is determined, which has the most inlier points. The number of iterations is represented by k, as shown in \eqref{equ:RANSAC repeatation}.

\begin{equation}
\frac{|Ax+By+Cz+D|}{\sqrt{A^2+B^2+C^2}}<T
\label{equ:inliers}
\end{equation}



\begin{equation}
k=\frac{\log(1-p)}{\log(1-w^n)} 
\label{equ:RANSAC repeatation}
\end{equation}
Where $1-w^n$ is the probability that at least one of the n points is failing the computation of outliers, $1-p$ is the probability that is independently selecting a set of n points are not all inliers. The maximum number of iterations can be set to stop the process.

In the sampling stage, some sampling errors are eliminated by setting the tilt angle threshold (as in \eqref{formula:inclination angle}) of the ground plane. The perpendicular plane model is adopted to restrict the wall-like vertical plane by checking the normal direction using the $diff2plane$ function. Even though the proposed ROI restricts the vertical height range to eliminate the error-prone planes (e.g., table surfaces), the distance of the detected plane from the origin is checked to make detection results more reliable. 


\begin{equation}
\theta=\arccos{\frac{|B|}{\sqrt{A^2+B^2+C^2}}}
\label{formula:inclination angle}
\end{equation}

\subsubsection{Feedback calibration}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/CNNfalse.png}
    \caption{The "2D" object detector treats an image on a piece of newspaper on the ground as a dangerous obstacle.}
    \label{fig:cnn-false}
\end{figure}

As mentioned in section \ref{section:orientation tracker}, the real-time height estimation is performed after the coefficients of the ground plane were determined. The height is calibrated during movement by updating the transform matrix using the intersection D of the plane. At the same time, we also update the transform matrix to ensure the normal vector n of the ground is parallel to (0,1,0). Therefore, the long-term drift of the gyroscope is further reduced. The closed-loop feedback mechanism makes the system robust when used for long periods under walking conditions.
\begin{figure}[b]
    \centering
    \includegraphics[width=\linewidth]{images/objectdetection.png}
    \caption{The procedure of the CNN-based object detection.}
    \label{fig:objectdetection_procedure}
\end{figure}

\begin{comment}
\subsubsection{Ground segmentation}
Although RANSAC is a guaranteed algorithm, the number of inliners depends on the number of iterations it runs. A seeded region growing algorithm is applied to save computation time and expand the traversable area longer and broader. The calculation of the normal vector of the point cloud is solved using the Principal Component Analysis. For each point and K neighborhood groups, the third component obtained from the analysis corresponds to the normal direction, i.e., flipped towards the viewpoint. Then, the growth starts from the seed pixel, which is chosen according to the preliminary RANSAC-based ground detection. The growing process is shown in Algorithm \ref{alg:regiongrowing}. Finally, the short-range ground plane has been enlarged to a longer and broader traversable area.




\begin{algorithm}
\caption{}
\label{alg:regiongrowing}

\KwIn{\\
$c$: input point cloud \\
$seed$: a point where the growth starts \\
$n$: number of neighbors \\
$c_{th}$: curvature threshold\\
$\theta_{th}$: angle threshold}     
\KwOut{\\
$G$:The  parameters  of  the  ground  plane} 
\textbf{procedure} RegionGrowing($c, n, seed, c_{th}, theta_{th}$) \\
\Indp
\textbf{initialization:}  \\
\quad $kd\_tree(c), G\leftarrow \emptyset $\\
\quad ${S} \leftarrow \emptyset$ \Comment*[]{Current seeds list}
$N=normalEstimate(c)$\\
$C=curvatureEstimate(c)$ \\
append $seed$ to ${S}$ \\
\While {${S}$ is not empty}{
	retrieve a seed $s$ from the list $S$ \\
		using $Kd\_tree$ to find nearest n neighbors points $P$ of $s$  in $c$ \\
		\For{point $p$ in $P$}{
			\If{$p$ is not visited and $\cos^{-1}(|N\{s\},N\{p\}|)<\theta_{th}$}{
				append $p$ to $G$ \\
				\If{$C\{p\}<c_{th}$}{
					append $p$ to ${S}$}
			}
		}
		remove $s$ from $S$
	
}
\textbf{return} $G$
\end{algorithm}

\end{comment}



\subsection{Object Detection}
To make the system reliable, we present two ways to detect objects: the CNN based object recognition and the imminent danger detection.


\subsubsection{CNN based object recognition}

Based on the investigation of the trade-off between speed and accuracy for modern convolutional object detectors \cite{huang2017speed}, we choose the combined detector, SSD\_MobileNet\_V2, as the object recognizer running on the mobile device.
By using depthwise separable convolutions to reduce both numbers of parameters and computational cost, the MobileNet\-V2
\cite{sandler2018mobilenetv2} archives a better balance between lightweight and performance than other networks \cite{ren2015faster} \cite{liu2016ssd} \cite{howard2017mobilenets} \cite{zhang2018shufflenet}. The detector uses MobileNet\-V2 as the feature extractor from images, then generates anchors using SSD \cite{liu2016ssd} with all the regular convolutions replaced by depthwise separable convolutions. The detector is trained on the Common Objects in Context (COCO) dataset\footnote{\url{http://cocodataset.org/}} from Microsoft, which includes 91 classes (e.g., person, chair, table), to help visually impaired people have a general perception of surroundings. However, the "2D" object detector cannot provide the distance information for blind navigation. 



Therefore, the result of the CNN object detector is fused with the depth map to get the distance information. The distance of the detected object is computed by averaging the value in the corresponding depth map area. Thus, rich information about surrounding objects is provided, such as the category and the distance. Because the depth is not considered in advance, a painted object on the ground might still be detected as an obstacle, which will lead to visually impaired people making wrong decisions. (see Fig. \ref{fig:cnn-false}). In order to eliminate this false positive detection result, the ground area identified from the point cloud is mapped to the original image according to the transformation matrix, and the corresponding ground area is masked from the image. The procedure is shown in Fig. \ref{fig:objectdetection_procedure}.





\subsubsection{Imminent danger detection}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/FoV.png}
    \caption{Narrowed horizontal FoV search area.}
    \label{fig:Fov}
\end{figure}



The CNN based object detector is not able to recognize some objects, and they may pose a danger to the user.
Our system features an imminent danger (e.g., hanging objects) detection that can detect any potentially dangerous obstacles and output their 3D bounding box. The imminent danger detection is performed using Euclidean clustering on the 3D point cloud. Rather than utilizing whole point cloud as input, the points belong to the ground area are removed. Besides, a passthrough filter is performed to focus on specific areas. For example, the area within 1 meter of the user is considered, because obstacles in this range may pose a danger to the user. In addition, when the distance from the user is less than 0.5 meters, the detection area is further reduced in the horizontal field of view (FoV) (Fig. \ref{fig:Fov}). This strategy allows the algorithm to focus on the user's straight direction to reduce false alarms, such as the door frame might is detected as an obstacle when crossing the door. 
\begin{algorithm}
\caption{Hazard detection based on Euclidean clustering approach.}
\label{alg:EcludeanClustering}
% \LinesNumbered
\KwIn{\\
$c$: input point cloud \\
$min$: the minimum size of the cluster \\
$max$: the maximum size of the cluster\\
$t$: the search radium}     
\KwOut{\\
$obL$: obstacle bounding box list[$x\_min, y\_min, z\_min, x\_max, y\_max, z\_max$]} 
\textbf{procedure} HazardDetection($c, min, max, t$) \\
\Indp
\textbf{initialization:}  \\
\quad $kd\_tree(c), C \leftarrow \emptyset, Q\leftarrow \emptyset$\;
\For{point $i$ in $c$}{
	\If{$i$ is not visited}{
		append $i$ to $Q$\;} 
	\For{point $q$ in $Q$}{
		using $Kd\_tree$ to search neighbor points $P$ of $q$ within the radium of $t$ in $c$\;
		\For{point $p$ in $P$}{
			\If{$p$ is not visited}{
				append $p$ to $Q$\; 
			}
		}
	}
	\If{the size of $Q$ is between $min$ and $max$}{
		append $Q$ to $C$\;
	}
	reset $Q$\;
}
\For{$cluster$ in $C$}{
	 $ob$ $\leftarrow$ bounding$\_$box($cluster$)\;
	append $ob$ to $obL$\;
}
\textbf{return} $obL$\;
\end{algorithm}


The k-dimensional (three-dimensional in our example) tree is used for range search and nearest neighbor search. Euclidean cluster extraction divides the input point cloud into a cluster list, which represents the detected obstacles. As described in Algorithm \ref{alg:EcludeanClustering}, the output of a set of 3D bonding boxes illustrate the position and size of the identified obstacles.




\subsection{Acoustic Feedback}
As described in related work \cite{kayukawa2019bbeep} \cite{edworthy1991improving}, the human reaction of audible emergency warnings has been investigated. We had a series of people who are actually blind. They were the ones who suggested feedback abilities. This call occurred and that many of the goals were shaped on the potential end-user feedback. We also conducted studies on eye-folded people to design a sound-emission strategy for our system. The sound emission policy includes different levels are presented in Table. \ref{tab:acoustic pattern}.

\begin{figure}
    \centering
    \subfloat[]{{\includegraphics[width=0.45\linewidth]{images/urgent_rgb.png} }}%
    \subfloat[]{{\includegraphics[width=0.45\linewidth]{images/urgent_depth.png} }}%
    \caption{An example of the imminent danger detection. (a) A hanging object is very close to the user's front. (b) The imminent danger detector can detect this potential danger from the point cloud, even if CNN did not recognize it.}%
    \label{fig:urgent_danger_detection}
\end{figure}




The low emergency sound warns that a collision may occur between 0.3 and 1 meters. The beep sound with low tone is spatialized in the 3D audio environment to guild visually impaired people. From the 3D reconstruction module, the user's position and orientation in the world coordinate were acquired. The output from speakers is determined by the relative position of the detected object to the user. One example is illustrated in Fig \ref{fig:3Daudio}. The user is oriented along the positive Z-axis, and his head is pointing towards up. If the position of the detected object on the X-axis has a positive value, the sound emitted from the location of the object will be heard from the left speaker. The volume of the sound emitted is controlled by attenuation (a multiplicative factor). The higher the attenuation, the less sound the user hears. If a warning occurs, the user should slow down the walking speed to continue.

\begin{table}[t]
\centering
\caption{Acoustic feedback patterns.}
\label{tab:acoustic pattern}
\begin{tabular}{|c|c|c|c|}
\hline
Level   & Type  & Pattern                & Time \\ \hline
prompt  & human & "1 chair 3 meters ahead" & 5s   \\ \hline
warning & beep  & 3D audio with low-key             & 1s   \\ \hline
urgent  & human & "no path found"          & 3s   \\ \hline
urgent  & beep  & high-key              & 1s   \\ \hline
\end{tabular}
\end{table} 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/3Daudio.png}
    \caption{3D audio feedback.}
    \label{fig:3Daudio}
\end{figure}
If the detected obstacle is within 0.3 meters or no available path is identified, the system will continue to issue an alarm to notify the user. In this case, the user should stop and then turn left / right to search for the best walking direction until the warning stops. In addition to providing warnings, voice prompts are used for users to improve their situational awareness. For example, if the user enters an unfamiliar environment, the user can activate the system to convert the result of the CNN object detector into speech to describe the scene to the user.



\section{Experimental Results and Discussions} \label{section:experiment}

 Experiments were conducted to evaluate the performance of the main modules in real indoor scenarios. Due to the COVID-19 pandemic, the final evaluation was only tested by blindfolded subjects for its quantitative and qualitative effectiveness in guiding users to walk safely. 


% \subsection{Experiments on Depth Pre-processing}
 



\subsection{Experiments on ground segmentation}
\subsubsection{ Orientation Tracking}

\begin{figure}
    \centering
    \begin{subfigure}{1\linewidth}
      \centering
      % include first image
      \includegraphics[width=0.7\linewidth]{images/pitch.png}
      \caption{}
    
    \end{subfigure}
    \begin{subfigure}{1\linewidth}
      \centering
      % include first image
      \includegraphics[width=0.7\linewidth]{images/Roll.png}  
      \caption{}
     
    \end{subfigure}

    \caption{The estimation of pitch and roll angles. Note: (a) is the pitch angle measurement; (b) is the roll angle measurement; $\alpha = 0.98$ for the complementary filter.}%
    \label{fig:anglestimation}
\end{figure}
Sensitivity, zero-offset, and bias are the intrinsic parameters of IMU, and they attempt to convert the raw sensor data into real measurements. These intrinsic parameters were calibrated to improve the accuracy of the angle estimation. First, IMU data in 6 different static positions (upright outward, upward and downward, etc.) were collected. We calibrated these parameters to ensure the accelerometer's output is consistent with the gravity decomposition on three axes. Then, the quantitative analyses of the real-time orientation tracker were performed. 

We evaluated the orientation tracker in a pre-set direction and compared the estimation results with the ground truth in three cases: using only the gyroscope, using complementary filters on the gyroscope and accelerometer, and applying feedback calibration to the complementary filters. The comparisons of pitch angle estimation and roll angle estimation are shown in Fig. \ref{fig:anglestimation}. The gyroscope based estimations could not be used in practice because of the time drift. The complementary filters illustrate good results; however, mechanical devices inevitably have biases and noises. Our feedback calibration expresses excellent results, because similar to the Kalman filter's mechanism, another discrete resource is invoked to correct bias. 


\subsubsection{Ground Segmentation}

\begin{table}[t]
\centering
\caption{Parameters of RANSAC algorithm for ground segmentation.}
\label{tab:ground-segmentation-parameters}
\begin{tabular}{|c|c|}
\hline
Model Type        &  NORMAL PLANE    \\ \hline
Max iterations       &  200    \\ \hline
Input normal        &  True    \\ \hline
Distance threshold        &  0.1m    \\ \hline
Axis       &  (0, 1, 0)    \\ \hline
Angle threshold        &  $5^\circ$    \\ \hline
Max distance from origin        &  0.1m    \\ \hline
\end{tabular}
\end{table}


  As depicted in Fig. \ref{fig:groundsegmentation}, three situations in the test scene were tested to evaluate ground segmentation's performance. The first case is on the left, with no obstacles on the floor. The ground had been successfully segmented, but several spots were not included. We adjusted parameters using loose constraints of the distance and angle. The missing spots can be segmented, but the whole ground area invaded to the object surface adjacent to the ground.  Since the floor near the door in the test scene is not flat, and the in distance area does not affect the whole system performance. We set the angle threshold to 5 degrees and the distance threshold to 0.1 meters to get balance results. The parameters of the RANSAC algorithm are listed in Table. \ref{tab:ground-segmentation-parameters}. These parameters are selected according to the performance of the segmentation, that is, including as many true-positive points as possible and few false-positive points. Then, scenes with obstacles in the distance and nearby were evaluated. We can see that the proposed algorithm has robust segmentation results in these cases. 

\begin{figure}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/openspace.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/normalspace.png} }}%
    \subfloat[]{{\includegraphics[width=0.32\linewidth]{images/crowedspace.png} }}%
    \caption{Results of the ground segmentation. (a) Open space without obstacles. (b) Space with obstacles in the distance. (c) Space with an obstacle nearby. Note: The blue region represents the segmented ground.}%
    \label{fig:groundsegmentation}
\end{figure}

\subsection{Experiments on Object Detection}


 As shown in Table. \ref{tab:cnn parameter}, the default configuration of SSD\_MobileNet\_V2 was utilized. The depth multiplier was applied to scale the number of channels in each layer. The minimum depth parameter ensures that after applying the depth multiplier, all layers have that many channels. To provide regularization and reduce generalization errors, we used L2 regularizer and batch normalization. Finally, the RMSprop optimizer with momentum was applied to classification loss and localization loss for training.
 

During reference, the confidence threshold was set to 0.5 to filter out low-confidence results. The point clouds were downsampled to 160 by 120 to imminent danger detection to reduce the computation cost. The distance threshold for point cloud clustering was set to 0.2 meters to separate point clusters. 


%% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\centering
\caption{Hyperparameters of SSD\_MobileNet\_V2  for object detection on images.}
\label{tab:cnn parameter}
\begin{tabular}{|c|c|c|}
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Architecture\\ hyperparameters:\end{tabular}} & depth multiplier                  & 1                                                                                                                            \\ \cline{2-3} 
                                                                                         & minimum depth                     & 16                                                                                                                           \\ \cline{2-3} 
                                                                                         & L2 regularizer                    & weight: 0.00004                                                                                                             \\ \cline{2-3} 
                                                                                         & batch normalization               & \begin{tabular}[c]{@{}c@{}}decay: 0.997\\ epsilon: 0.001\end{tabular}                                                       \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Training \\ hyperpereters:\end{tabular}}      & input size                        & 300 * 300                                                                                                                    \\ \cline{2-3} 
                                                                                         & batch size                        & 24                                                                                                                           \\ \cline{2-3} 
                                                                                         & \multirow{2}{*}{RMSProp Optimizer} & \begin{tabular}[c]{@{}c@{}}exponential decay- \\ learning rate: 0.004\\ decay steps: 800720\\ decay factor: 0.95\end{tabular} \\ \cline{3-3} 
                                                                                         &                                   & \begin{tabular}[c]{@{}c@{}}momentum optimizer- \\ value: 0.9\\ decay: 0.9\\ epsilon: 1\end{tabular}                           \\ \hline
\end{tabular}
\end{table}
Three scenarios were tested in an indoor environment to verify our object detection method. As shown in Fig. \ref{fig:objectdetection}, we first tested the ability to detect large and small everyday objects. In cases 1 and 2, imminent danger detection from point clouds and CNN-based object detection from images can successfully identify the bottle and the chair. For exceptional cases, a medium-sized unusual object was tested. The CNN model could not identify the object because there was no category with a confidence score higher than 0.5. However, our imminent danger detection method could still detect the missing object. Regardless of the size and type of object, the imminent danger detection is a reliable method. It can identify objects that are missing from machine learning-based methods and ensure that the object detection can be performed reliably by blind people.







% to test whether object detection can help visually impaired people travel more effectively. According to the urgent detection results in our system, users can hear 3D low-key beeps from two speakers according to the sound emission pattern. However, urgent detection does not provide category information about blocked objects. The users must turn left or right to search for the direction of movement. If users activate CNN-based object detection, the human speech will describe more information about the surrounding environment. In this case, the users know the object and can push the chair away to prevent them from taking more detours and improve their environmental awareness.

\begin{figure}
    \centering
    \captionsetup[subfigure]{labelformat=empty}
    \subfloat[case 1(a)]{{\includegraphics[width=0.32\linewidth]{images/originalbottle.png} }}%
    \subfloat[case 1(b)]{{\includegraphics[width=0.32\linewidth]{images/bottle.png} }}%
    \subfloat[case 1(c)]{{\includegraphics[width=0.32\linewidth]{images/CNNbottle.png} }}%
    \qquad
    \subfloat[case 2(a)]{{\includegraphics[width=0.32\linewidth]{images/original.png} }}%
    \subfloat[case 2(b)]{{\includegraphics[width=0.32\linewidth]{images/chairdep.png} }}%
    \subfloat[case 2(c)]{{\includegraphics[width=0.32\linewidth]{images/CNN_result.png} }}%
    \qquad
    \subfloat[case 3(a)]{{\includegraphics[width=0.32\linewidth]{images/resultcan.png} }}%
    \subfloat[case 3(b)]{{\includegraphics[width=0.32\linewidth]{images/trashcan.png} }}%
    \subfloat[case 3(c)]{{\includegraphics[width=0.32\linewidth]{images/CNNcan.png} }}%
    \caption{Results of the object detection. Case 1: Small object (a bottle) detection; Case 2: Large object (a chair) detection; Case 3: Unrecognized object by the machine learning model. Note: (a)s are RGB images; (b)s are results of the imminent danger detection from point clouds, the direction is left and right  reversed relative to RGB images; (c)s are results of the CNN-based object detection from images; The green regions represent the segmented ground from point clouds; The blue regions represent the segmented ground in images.}%
    \label{fig:objectdetection}
\end{figure}

\subsection{Experiments on ACOUSTIC FEEDBACK}

Acoustic feedback is the main method of interaction between the prototype system and the end-user. We used the text-to-speech to provide informative human voice feedback and a 3D audio engine to emit spatial beeps. The designed scenario of Fig. \ref{fig:objectdetection} case 2 (where the way was blocked by a chair) was used to evaluate the sound emission policy and trigger conditions. In this test, we assumed that the blindfolded person 
was trying to reach the destination through this blocked path.


During the system setup process, the system prompted "The system is setting" to notify the user to remain static. After initialization, the system informed the user of the estimated height of the camera above the ground. If the user judged that the height was incorrect, the setting process could be restarted. When the chair was detected within 1 meter, the 3D position of the estimated bounding box centroid of the chair and the height of the camera were used as parameters for invoking the 3D audio engine. Since the volume and sound channel of the beep sound depend on the relative position of the chair and the camera, the user could obtain the spatial information of the chair. With the subject continue advancing to the chair within 0.3 meters, the system beeped with a high tone to warn of an impending collision. In addition, from the camera's point of view, since the chair occupied most of the scene and the ground could not be detected, the "Path not found" was issued. At this point, the user should immediately stop and change the direction to search for available paths. However, the user could activate the CNN-based object detection, and the text-to-speech engine converted the results of the CNN object detector into speech ("chair 0.3 meters ahead"). Therefore, the subject could simply move the chair and continue to move. This can prevent the visually impaired from detouring and increase their environmental awareness. Hence, our dedicated audio feedback can help visually impaired people improve travel efficiency and bring them a better travel experience.



\subsection{Computational Cost}

The hardware includes an Intel RealSense D435i camera\footnote{https://www.intelrealsense.com/depth-camera-d435i/}, a Raspberry Pi model b with 4Gb memory\footnote{http://www.raspberrypi.org/products/raspberry-pi-4-model-b/}, and an ANKER PowerCore 10000 mobile power supply\footnote{http://www.anker.com/products/variant/powercore-10000-pd/A1235011}. Point Cloud Library (PCL) version 1.9.0 is used to process point clouds, and Open Multi-Processing (OpenMP) is applied to achieve multi-threaded parallelism. The machine learning model for object detection is optimized using quantization to reduce the reference time. In the sound feedback module, the Flite text-to-speech engine\footnote{http://www.festvox.org/flite/} and the Open AL audio engine\footnote{https://www.openal.org/} are adopted to provide human speech and 3D audio feedback.

All algorithms run on the local Raspberry Pi. As depicted in Table. \ref{tab:computation cost}, the average computational time among 100 epochs for the three cases in Fig. \ref{fig:objectdetection} were calculated. We show that the time spent on ground segmentation and imminent danger detection has a positive correlation with the size of the object. Compared with case 1, case 2 requires more than 26\% of the time to segment the ground. Because the smaller the percentage of the ground area in the regional proposal, the more time it takes to segment it from random sampling. Besides, due to the need to cluster more points for larger objects, case 2 takes 2.03 milliseconds, 3.44 times longer than case 1. Since CNN-based object detection is only activated when a visually impaired user wishes to understand the surrounding environment, the total time of all algorithms is about 358 milliseconds on average. Therefore, the proposed system can provide real-time assistance to the visually impaired in typical walking situations.

\begin{table}[]
\centering
\caption{Average running time among 100 epochs. Note: The total and average time are calculated with CNN-based object detection excluded.}
\label{tab:computation cost}
\begin{tabular}{|c|c|c|c|}
\hline
Stage                      & Case 1  & Case 2 & Case 3 \\ \hline
Depth preprocessing        &    114.34 ms  &  118.32 ms & 120.70 ms\\ \hline
3D scene reconstruction    &   61.57 ms   & 61.74 ms & 60.28 ms\\ \hline
Normal estimation          &    37.88 ms& 39.92 ms &  39.87 ms \\ \hline
Ground segmentation            &   122.91 ms  & 155.11 ms & 148.57 ms \\ \hline
Immient danger detection &     0.59 ms&  2.03 ms &  1.06 ms \\ \hline
CNN-based object detection       &   65.89 ms  &  70.21 ms & 70.52 ms\\ \hline
Total & 337.29 ms& 365.12 ms & 370.48 ms\\ \hline
Average &\multicolumn{3}{c|}{357.68 ms}  \\ \hline
\end{tabular}
\end{table}



\subsection{Discussions}
\begin{figure*}[t]
    \centering
    \subfloat[]{{\includegraphics[width=0.31\linewidth]{images/SegCompare1.png} }}%
    \subfloat[]{{\includegraphics[width=0.31\linewidth]{images/SegCompare2.png} }}%
    \subfloat[]{{\includegraphics[width=0.31\linewidth]{images/SegCompare3.png} }}%
    \caption{Comparison of time spent on ground segmentation among 100 epochs (including normal estimation). (a) Testing on the scenario Fig \ref{fig:groundsegmentation}(a). (b) Testing on the scenario Fig \ref{fig:groundsegmentation}(b). (c) Testing on the scenario Fig \ref{fig:groundsegmentation}(c).}%
    \label{fig:comparison}
\end{figure*}
Through quantitative and qualitative evaluations, our wearable system effectively guides visually impaired people to avoid obstacles in indoor environments using low-power embedded devices. 
Different from works \cite{takizawa2019kinect} \cite{yang2016expanding} that segment ground in the whole search space, or search the area below the estimated ground height \cite{bai2019wearable}, we propose a novel architecture that can segment the ground on the proposed region based on real-time orientation and altitude tracker. This method reliably proposes the ground ROI and greatly reduces the 3D search space of the RANSAC algorithm. The performance improvement is highly dependent on the concrete scenes because of the irregularity of the point cloud. We compared the ground segmentation time in the whole space and the proposed region of the three typical indoor scenes in Fig.\ref{fig:groundsegmentation}. In scenario (a), the average time of region-based segmentation is shortened by an average of 73.73\%, in scenario (b) by 50.85\%, and in scenario (c) by 57.78\%. The results are depicted in Fig. \ref{fig:comparison}, which demonstrates the efficiency of our method on ground segmentation using a low power device.

The object detection from the point cloud is a guaranteed method that can reliably detect any potential hazards. Still, it is not efficient to identify the category of objects through modeling. Machine-learning-based methods can provide the category information about detected objects, but they may miss objects, although they may still pose a danger. Another advantage of our approach is that object detection is performed from point clouds and images. Such unrecognized objects can be detected by the imminent  danger detector to help users avoid collisions. Simultaneously, machine learning methods can be used to improve the situational awareness of blind people. Since machine learning is 
a data-driven model that predicts objects based on the trained data, we can retrain the machine learning model by capturing images of the target environment to improve the accuracy. 

Instead of sensing the reflected sound from the parametric speaker \cite{takizawa2019kinect} to help users understand the angle of the object, we use 3D audio feedback to provide not only the direction but also the distance information of the object. Besides, if the object's surface is not flat, the parametric speaker may not provide the correct guidance. Our system can help build a more accurate understanding of the surrounding environment by offering objects with a sense of distance and direction to avoid obstacles.

The system runs on the Raspberry Pi at about 3 Hz in real time. By overclocking the CPU frequency to a maximum of 2 GHz, the operating speed is slightly increased. By further downsampling the point cloud to a smaller size, the processing time can be reduced, which is a compromise between detection accuracy and speed. We also tested the system on a desktop computer with an Intel 8700K CPU and 16 Gb RAM, and the results showed an approximately 10-fold increase in update speed. Although the hardware prototype is less powerful, it consumes less power and can still provide timely feedback.


Our method still has limitations though. The neural network is only trained for common objects. For special objects, it cannot provide the correct semantic category information for the end-user, for example, case 3 in Fig. \ref{fig:objectdetection}. Since we do not search the space under the ground in the point cloud, the imminent danger detector is almost impossible to detect down-stairs. The acoustic feedback may distract blind people because the sound is the main source of blind people's perception of the environment.


\section{Conclusions and Future Work} \label{section:conclusion}
This article proposes a wearable device for indoor imminent danger detection that can assist people with severe visual impairments to move around independently. The system features include wayfinding, imminent danger detection, object recognition, and 3D acoustic feedback. The user's surroundings can be reconstructed accurately by tracking the orientation and altitude of the camera. Thus, we proposed a region-based ground segmentation, which reduces the computational cost and increases robustness. The imminent danger detector provides a guaranteed method for identifying obstacles that are unrecognizable by the machine learning method but still might pose a danger. By utilizing an optimized 3D audio feedback mechanism based on the detected potential threats, our system can help users prevent imminent hazards and improve their situational awareness.

 
We conducted thorough experimental evaluations in common indoor environments. Experimental results prove that our effective technology can be a useful tool to help blind people reduce travel injuries. Due to the COVID-19, the university was closed. It was difficult to invite blind people to campus for system evaluation. Therefore, the final integrated system was only tested by blindfolded people. In the future, we plan to apply deep neural network models directly on the 3D point cloud to estimate object-oriented 3D bounding boxes and semantic categories to improve performance further. In addition, more evaluations will be conducted, and a tactile wrap will be added to the wrist to improve the feedback mechanism.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{acl2017}
\bibliographystyle{IEEEtran}

\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/zhongen.png}}]{Zhongen Li} received the B.S. degree in information and computing science from Anhui Jianzhu University, Hefei, China, in 2013 and the M.S. degree in integrated circuit engineering from University of Science and Technology of China, Hefei, China, in 2016. He is currently pursuing the Ph.D. degree in computer engineering at Ohio University, Athens, OH, USA. His current research interests include object detection in 2D and 3D, and natural language processing.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/fanghao.png}}]{Fanghao Song} 
received the B.S. degree in heat energy dynamics engineering (thermal engineering) from Shenyang Aerospace University, Shenyang, China, in 2014 and the M.S. degree in mechanical engineering from the University of Southern California, CA, USA, in 2016. He is currently pursuing the Ph.D. degree in computer science at Ohio University, Athens, OH, USA. His current research interests focus on health applications and indoor navigation software development. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Clark.png}}]{BRIAN C. CLARK} received the B.S. degree in biology from Western Carolina University in Cullowhee, NC, USA in 1998, a M.S. degree in exercise physiology (2001) and a Ph.D. in neuromuscular physiology (2005) from Syracuse University in Syracuse, NY, USA.

He is the Clybourne Endowed Research Chair and Professor of Physiology and Neuroscience in the Department of Biomedical Sciences at Ohio University, Athens, OH, USA where he also is a Principal Investigator and Executive Director of the Ohio Musculoskeletal and Neurological Institute. The overall goal of his research is to develop effective and implementable interventions that increase muscle function and mobility in patients with orthopedic and neurologic disabilities. He has published more than 135 peer reviewed articles in clinical, basic science, and engineering journals.

Dr. Clark’s awards and honors include receiving the All-University Doctoral Prize from Syracuse University (2006), serving on dozens of national and international grant review panels including being a stand member of the NIH’s Motor Function and Speech Rehabilitation Study Section (2014-2020), and serving on numerous expert panels and round tables for federally- and industry sponsored projects.
 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Grooms.png}}]{Dustin R. Grooms} received bachelor’s degree in Athletic Training from Northern Kentucky University, Highland Heights, KY, USA, in 2008 and the M.S. degree in kinesiology from the University of Virginia, Charlottesville, VA, USA, in 2009, and the Ph.D. degree in health and rehabilitation sciences from the Ohio State University, Columbus, OH, USA, in 2015. 

Prior to pursuing doctoral studies, he was a rehabilitation clinician and instruct at the College of Mount Saint Joseph in Cincinnati Ohio. He is an Associate Professor in the Division of Athletic Training in the School of Applied Health Sciences and Wellness at Ohio University. His current main research interest is how the brain and movement mechanics change after injury and the design of novel therapies.


\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{images/Liu_Chang.png}}]{Chang Liu} received the Ph.D. degree in Information and Computer Science from the University of California, Irvine, CA, USA, in 2002. 

He is an IEEE senior member and currently a professor in computer science at Ohio University. His research focuses on software engineering and its applications in various domains, including learning and medical applications. 
\end{IEEEbiography}

\EOD

\end{document}
